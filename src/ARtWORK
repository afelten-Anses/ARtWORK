#!/usr/bin/python
# -*- coding: iso-8859-1 -*-
import os, sys
import subprocess 
import datetime
import time
import argparse
import glob
from pyunpack import Archive 	
import shutil
import pymongo
from pymongo import MongoClient
from bson.objectid import ObjectId
from shutil import copyfile
import smtplib
from Bio import SeqIO
import patoolib


__doc__="""

"""

def get_parser():
	"""
	Parse arguments
	@return: arguments list
	@rtype: parser object
	"""

	parser = argparse.ArgumentParser(description='run reads analysis, variant analysis, assembly and annotation \
		from raw reads and add outputs in the GAMeR database')

	parser.add_argument('-1', action="store", dest='reads1',
						type=str, required=True, help='Fastq file pair 1 with \'_R1\' (REQUIRED)')

	parser.add_argument('-2', action="store", dest='reads2',
						type=str, required=True, help='Fastq file pair 2 with \'_R2\' (REQUIRED)')

	parser.add_argument('-g', action="store", dest='genus',
						type=str, required=True, help='genus Salmonella/Listeria/Staphylococcus (REQUIRED)')

	parser.add_argument('-s', action="store", dest='species',
						type=str, required=True, help='species (REQUIRED)')

	parser.add_argument('-minCov', action="store", dest='minCov',
						type=int, default=30, help='minimum coverage required for workflow (default:30)')

	parser.add_argument('-targetCov', action="store", dest='targetCov',
						type=int, default=100, help='minimum coverage required for normalization, 0 for no normalization (default:100)')

	parser.add_argument('-l', action="store", dest='minContigLen',
						type=int, default=200, help='minimum contig length (default:200)')

	parser.add_argument('-p', action="store", dest='minPhredScore',
						type=int, default=30, help='minimum phred score (default:30)')

	parser.add_argument('-T', action="store", dest='nbThreads', 
						type=int, default=1, help='maximum number of threads to use (default:1)')

	parser.add_argument('-m', action="store", dest='maxMemory', 
						type=int, default=4000, help='max memory to use in Mb (default:4000)')

	parser.add_argument('-w', action="store", dest='workdir', 
						type=str, default='.', help='working directory (default:current directory)')

	parser.add_argument('--supplier', action="store", dest='supplier', 
						type=str, default='Anses', help='Supplier (default:Anses)')

	parser.add_argument('--project', action="store", dest='projetID', 
						type=str, required=True, help='Project id')

	parser.add_argument('--center', action="store", dest='center', 
						type=str, required=True, help='sequencing center')

	parser.add_argument('--technology', action="store", dest='technology', 
						type=str, required=True, help='technology used for sequencing')


	return parser



class sample(object) :


	def __init__(self, SampleID, supplier, projetID, report, \
		reads1Path, reads2Path, readsLen, reads1QC, reads2QC, center, technology, \
		assembly, contigs, QUAST, GBK, GFF, \
		genus, species, ST, MLST_id, VCF):
		"""
		Initialize the sample class
		"""

		self.id = SampleID	
		self.supplier = supplier
		self.projetID = projetID
		self.report = report

		self.nbReads = number_of_reads(reads1Path)
		self.reads1 = self.compressReads(reads1Path)
		self.reads2 = self.compressReads(reads2Path)
		self.readsLen = readsLen
		self.reads1QC = reads1QC
		self.reads2QC = reads2QC
		self.center = center
		self.technology = technology
		self.VCF = VCF

		self.assembly = assembly
		self.contigs = contigs
		self.QUAST = QUAST
		self.GBK = GBK
		self.GFF = GFF

		self.genus = genus
		self.species = species
		self.ST = ST

		if self.genus == "Salmonella" :
			self.serovar = MLST_id

		elif self.genus == "Listeria" :
			self.CC = MLST_id 	


	def moveFiles(self, FINAL_OUTPUT_DIRECTORY):

		mongo_output = '/'.join(FINAL_OUTPUT_DIRECTORY.split('/')[4:]) + '/'

		os.system("mv " + self.report + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")

		self.report = mongo_output + self.report.split("/")[-1]
		
		os.system("mv " + self.reads1 + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads1 = mongo_output + self.reads1.split("/")[-1]

		os.system("mv " + self.reads2 + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads2 = mongo_output + self.reads2.split("/")[-1]

		os.system("mv " + self.reads1QC + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads1QC = mongo_output + self.reads1QC.split("/")[-1]

		os.system("mv " + self.reads2QC + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads2QC = mongo_output + self.reads2QC.split("/")[-1]

		os.system("mv " + self.assembly + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.assembly = mongo_output + self.assembly.split("/")[-1]

		os.system("mv " + self.contigs + ' ' + FINAL_OUTPUT_DIRECTORY + "/" + self.id + "_contigs.fasta")
		self.contigs = mongo_output + self.id + "_contigs.fasta"

		os.system("mv " + self.QUAST + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.QUAST = mongo_output + self.QUAST.split("/")[-1]

		os.system("mv " + self.GBK + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.GBK = mongo_output + self.GBK.split("/")[-1]

		os.system("mv " + self.GFF + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.GFF = mongo_output + self.GFF.split("/")[-1]

		os.system("mv " + self.VCF + '* ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.VCF = mongo_output + self.VCF.split("/")[-1]


	def compressReads(self, reads):	

		if reads.split('.')[-1] != "gz" :
			os.system("pigz " + reads)
			reads = reads + ".gz"

		return reads	


	def insertMongo(self):
		
		client = MongoClient('localhost', 27017)
		db = client.GAMeRdb
		genomes = db.GENOME

		document = {
			"SampleID":self.id,
			"Supplier":self.supplier,
			"Project":self.projetID,
			"Report":self.report,
			"Reads":{
				"FASTQ_pair1":self.reads1,
				"FASTQ_pair2":self.reads2,
				"NbReads":self.nbReads,
				"ReadsLength":self.readsLen,
				"FASTQC_pair1":self.reads1QC+"/fastqc_report.html",
				"FASTQC_pair2":self.reads2QC+"/fastqc_report.html",
				"Center":self.center,
				"Technology":self.technology,
				"VCF":self.VCF
			},
			"Genome":{
				"Assembly":self.assembly,
				"Contigs":self.contigs,
				"QUAST":self.QUAST+"/report.html",
				"GBK":self.GBK,
				"GFF":self.GFF
			},
			"Phylogeny":{
				"Genus":self.genus,
				"Species":self.species,
				"SequenceType":self.ST,
			}
		}

		if self.genus == "Salmonella" :
			document["Phylogeny"]["Serovar"] = self.serovar

		elif self.genus == "Listeria" :	
			document["Phylogeny"]["ClonalComplex"] = self.CC	


		post_id = genomes.insert_one(document).inserted_id	


	def debug(self):	

		print "Genome id : " + self.id
		print "Supplier : " + self.supplier
		print "Projet id : " + self.projetID
		print "Report path : " + self.report

		print "Reads 1 path : " + self.reads1
		print "Reads 2 path : " + self.reads2	
		print "Nb reads : " + str(self.nbReads)
		print "Read length : " + str(self.readsLen)
		print "Reads 1 path : " + self.reads1
		print "FASTQC reads 1 path : " + self.reads1QC
		print "FASTQC reads 2 path : " + self.reads2QC
		print "Sequencing center : " + self.center
		print "Sequencing technology : " + self.technology
		print "VCF file : " + self.VCF

		print "Assembly path : " + self.assembly
		print "Contigs path : " + self.contigs
		print "QUAST path : " + self.QUAST
		print "GBK path : " + self.GBK
		print "GFF path : " + self.GFF

		print "Genus : " + self.genus
		print "Species : " + self.species
		print "ST : " + self.ST

		if self.genus == "Salmonella" :
			print "Serovar : " + self.serovar

		elif self.genus == "Listeria" :	
			print "Clonal Complex : " + self.CC



def Get_maxMemoryg(maxMemory):

    #1go to 10go RAM conversion
    if(int(maxMemory) >= 1000 and int(maxMemory) < 10000):
        maxMemoryg = "-Xmx" + list(str(maxMemory))[0] + "g"

    #10go to 100go RAM conversion
    elif(int(maxMemory) >= 10000 and int(maxMemory) < 100000):
        maxMemoryg = "-Xmx" + str(list(str(maxMemory))[0] + list(str(maxMemory))[1]) + "g"

    #100go to 1000go RAM conversion
    elif(int(maxMemory) >= 100000 and int(maxMemory) < 1000000):
        maxMemoryg = "-Xmx" + str(list(str(maxMemory))[0] + list(str(maxMemory))[1] + list(str(maxMemory))[2]) + "g"

    #1To to 10To RAM conversion
    elif(int(maxMemory) >= 1000000 and int(maxMemory) < 10000000):
        maxMemoryg ="-Xmx" + str(list(str(maxMemory))[0] + list(str(maxMemory))[1] + list(str(maxMemory))[2] + list(str(maxMemory))[3]) + "g"

    #1go is the minimum requierement to be able to some tools, so if arguments.nbThreads<1go, enter Arguments.nbThreads converted defaut value
    else:
        maxMemoryg="-Xmx4g"
        
    return maxMemoryg
	

def check_finalDirectory_exist(FINAL_OUTPUT_DIRECTORY, logFilepath):
	
	logFile = open(logFilepath, "a")
	logFile.write("Check if final directory exists...")

	if(os.path.isdir(FINAL_OUTPUT_DIRECTORY) ==True):

		Gid = FINAL_OUTPUT_DIRECTORY.split('/')[-1]
		message = "ERROR:" + Gid + " already exist in " + FINAL_OUTPUT_DIRECTORY 

		#add msgerr in logFile
		logFile = open(logFilepath, 'a')
		logFile.write(message + '\n') 
		logFile.close()
		sys.exit(1)

	logFile.write("...ok\n")
	logFile.close()




def check_MongoDB_exist(GENOME_ID, genus, logFilepath):
	
	logFile = open(logFilepath, "a")
	logFile.write("Check if genome already id exists in  database...")

	client = MongoClient('localhost', 27017)
	db = client.GAMeRdb
	genomes = db.GENOME

	if genomes.find({"SampleID":GENOME_ID , "Phylogeny.Genus":genus}).count() > 0:  #fix : if count > 0, genome exists in database

		message = "ERROR:" + GENOME_ID + " already exist in the database!!!"

		#add msgerr in logFile
		logFile = open(logFile, 'a') 
		logFile.write(message + '\n')
		logFile.close()
		sys.exit(1)

	logFile.write("...ok\n")
	logFile.close()


def uncompress_in_workdir(CompressedRead,logFilepath,workdir):


	################################################################
	## Extract reads_filename and file path from compressed file  ##
	################################################################

	os.system('echo "\n************************** READS UNPACKING : START **************************\n" >> '+ logFilepath)

	#File found
	if(os.path.exists(CompressedRead)):

		reads_filename = '.'.join(CompressedRead.split('.')[0:2])
		reads_filename = reads_filename.split('/')[-1]
		reads_filepath = workdir + '/' + reads_filename

	#File not found
	else:
		logFile = open(logFilepath, 'a')
		logFile.write("File not found : " + CompressedRead)
		logFile.close()
		sys.exit(1)
	

	##################################################
	##    Test and unpack archive if test is ok     ##
	##################################################

	### TEST ###

	os.system('echo "unpacking started at :" >> ' + logFilepath + ' 2>&1 | date >> ' + logFilepath + ' 2>&1')

	try:
		patoolib.test_archive(CompressedRead,verbosity=0)

	except:
		logFile = open(logFilepath, 'a')
		logFile.write("Problem with file : " + CompressedRead)
		os.system('echo "unpacking stopped with status error 1 at" : >> ' + logFilepath + ' 2>&1 | date >> ' + logFilepath + ' 2>&1')
		is_corruputed=True
		sys.exit(1)

	
	### IF TEST=SUCESS --> UNPACK IN CURRENT WORKING DIRECTORY ###

	else:
		Archive(CompressedRead).extractall(workdir)
		is_corruputed=False

	os.system('echo "unpacking finished sucessfully at" : >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

	os.system('echo "\n************************** READS UNPACKING : END **************************\n" >> '+ logFilepath)

	return reads_filename,is_corruputed


def normalization(reads1, reads2, targetCov, nbThreads, Gid, logFilepath, reportFilepath):

	reads1_normalized = Gid + "_norm_R1.fastq"
	reads2_normalized = Gid + "_norm_R2.fastq"

	cmd = "bbnorm.sh -Xmx20g " + \
	"in=" + reads1 + " " + \
	"in2=" + reads2 + " " + \
	"target=" + str(targetCov) + " " + \
	"mindepth=-1 " + \
	"maxdepth=1 " + \
	"out=" + reads1_normalized + " " + \
	"out2=" + reads2_normalized + " " + \
	"outt=trashReads.fastq " + \
	"threads=" + str(nbThreads) + " " + \
	"prefilter=t " + \
	"tossbadreads=t" 

	logFile = open(logFilepath, "a")
	logFile.write("\nReads Normalization to " + str(targetCov) + "X\n")
	logFile.write(cmd + "\n")
	logFile.close()

	os.system('echo "bbnorm started at : " >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)
	os.system(cmd)
	os.system('echo "bbnorm ended at" : >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

	report_file = open(reportFilepath, "a")
	report_file.write("TARGET COVERAGE FOR NORMALIZATION : " + str(targetCov) + 'X\n\n')
	report_file.close()
	
	return reads1_normalized, reads2_normalized
    


def Check_coverage(reads1,reads2,genus,minCov,nbThreads,maxMemory,workdir,logFilepath,reportFilepath):
    
    os.system('echo "\n************************** QUALITY CONTROL (Check coverage) : START **************************\n" >> '+ logFilepath)

    #Convert maxMemory argument : Aaaa int to to -XmxAg form
    Maxmemoryg=Get_maxMemoryg(maxMemory)

    logFile = open(logFilepath, "a")
    logFile.write("Retrieving default reference...")

    #Get default reference before mapping accorded to this reference
    reference=get_default_reference(genus,workdir,logFilepath)

    logFile.write("...ok\n")

    os.system('echo "bbmap started at : " >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

    ##################################################
    ## Map the reads and generate its quality stats ##
    ##################################################
    cmd = "bbmap.sh " +\
    "in="+reads1+" "+\
    "in2="+reads2+" "+\
    "ref="+reference+" "+\
    "covstats=" + workdir +"/stats/covstats.txt "+\
    "covhist=" + workdir + "/stats/covhist.txt "+\
    "bincov=" + workdir + "/stats/bincov.txt "+\
    "rpkm=" + workdir + "/stats/rpkm_fpkm.txt "+\
    "ehist=" + workdir + "/stats/error_hist "+\
    "statsfile=" + workdir + "/stats/reads_summary.txt "+\
    "covbinsize=1000 " + \
    "k=13 "+\
    "threads="+str(nbThreads)+" "+\
    "nodisk "+\
    str(Maxmemoryg)+" "+\
    ">> " + str(logFilepath) + " 2>&1"

    logFile.write("\n" + cmd + "\n")
    os.system(cmd)

    os.system('echo "bbmap ended at" : >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

    #####################
    ## Quality control ##
    #####################
    logFile.write("quality control : check medium coverage")
    #si coverage < minCov ---> écrire message d'erreur dans stdin + report + os.exit(1)
    #ajouter coverage trouvé + std variantion dans report
    coverage=subprocess.check_output("cat " + workdir + "/stats/covstats.txt | cut -f2 | sed -n \"2 p\"", shell=True).rstrip().replace(',','.')
    coverage_std_var=subprocess.check_output("cat " + workdir + "/stats/covstats.txt | cut -f11 | sed -n \"2 p\"", shell=True)
    
    report_file = open(reportFilepath, "a")
    report_file.write("READS DEEP COVERAGE : " + coverage + '\n')
    report_file.write("COVERAGE STANDARD DEVIATION : " + coverage_std_var + "\n")
    report_file.close()

    if (float(coverage) < float(minCov)):
        logFile.write("\n Warning, too low coverage : " + coverage + "x")
        logFile.close()
        sys.exit(1)

    logFile.write("...ok\n")
    logFile.write("RPKM / FPKM : \n")
    os.system("cat stats/rpkm_fpkm.txt >> " + logFilepath)
    logFile.write("\nCoverage histogram : \n")
    os.system("cat stats/covhist.txt >> " + logFilepath)
    logFile.close()

    os.system('echo "\n************************** QUALITY CONTROL (Check coverage) : END **************************\n" >> '+ logFilepath)

    return int(float(coverage))


def soft_trimming(reads1, reads2, adaptaters, nbThreads, maxMemory, phredscore, Gid, workdir, report, logFilepath):
	
	os.system('echo "************************** TRIMMING WORKFLOW : START ************************** " >> '+ logFilepath)

	Maxmemoryg=Get_maxMemoryg(maxMemory)
	logFile = open(logFilepath, "a")

	##################################################
	## Map the reads and generate its quality stats ##
	##################################################

	logFile.write("Calculating number of reads per file...")
	nbReads_before_trimming = number_of_reads(reads1)
	logFile.write("...ok\n")

	forward_paired = workdir + "/" + Gid + "_paired_R1.fq"
	forward_unpaired = workdir + "/" + Gid + "_unpaired_R1.fq.gz"
	reverse_paired = workdir + "/" + Gid + "_paired_R2.fq"
	reverse_unpaired = workdir + "/" + Gid + "_unpaired_R2.fq.gz"

	os.system('echo "Trimmomatic started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	command = "java -jar " + str(Maxmemoryg) + " /opt/Trimmomatic/0.33/trimmomatic-0.33.jar PE -threads " + \
	str(nbThreads) + " -phred33 " + reads1 + " " + reads2 + " " + forward_paired + " " + \
	forward_unpaired + " " + reverse_paired + " " + reverse_unpaired + \
	" ILLUMINACLIP:" + adaptaters + ":2:" + str(phredscore) + ":15 " + \
	"TRAILING:20 MINLEN:50 >> " + logFilepath + " 2>&1"

	logFile.write("\n" + command + "\n")
	os.system(command)
	os.system('echo "Trimmomatic ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	#os.system("pigz " + forward_paired + ".gz")
	#os.system("pigz " + reverse_paired + ".gz")

	os.remove(forward_unpaired)
	os.remove(reverse_unpaired)

	logFile.write("Calculating number of reads per file after trimming...")
	nbReads_after_trimming = number_of_reads(forward_paired)
	logFile.write("...ok\n")
	logFile.close()

	report_file = open(report, "a")
	report_file.write("" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+            TRIMMING RESULTS           +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n")
	report_file.write("READS TRIMMING PARAMETERS : minQual = 20 -- minLen = 50 -- minPhred = " + str(phredscore) + "\n")
	report_file.write("READS COUNT BY FILE BEFORE TRIMMING : " + str(nbReads_before_trimming) + " \n")
	report_file.write("READS COUNT BY FILE AFTER TRIMMING : " + str(nbReads_after_trimming) + " \n")
	report_file.write("TRIMMED READS COUNT : " + str(int(nbReads_before_trimming-nbReads_after_trimming)) + "\n\n")
	report_file.close()

	list_reads_trim = [forward_paired, reverse_paired]
	
	os.system('echo "\n************************** TRIMMING WORKFLOW : END **************************\n" >> '+ logFilepath)
	return list_reads_trim


def Fastqc(reads1, reads2, adaptators, outputName, nbThreads, workdir, logFilepath, reportFilepath):

	#A mettre dans fichier REPORT : lien vers le report html

	os.system('echo "\n************************** FASTQC REPORT : START **************************\n" >> '+ logFilepath)

	os.mkdir(workdir + "/" + outputName)
	os.system('echo "FastQC started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	cmd = "fastqc -q -t " + str(nbThreads) + " --contaminants " + adaptators + \
		" -o " + workdir + "/" + outputName + " " + reads1 + " " + reads2 + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	os.system(cmd)

	os.system('echo "FastQC ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	#Full path to zip files
	TXTfastqcResult_R1 = workdir + "/fastqc/" + reads1.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc/fastqc_data.txt"
	TXTfastqcResult_R2 = workdir + "/fastqc/" + reads2.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc/fastqc_data.txt"

	#Full path to txt files
	ZIPfastqcResult_R1 = workdir + "/fastqc/" + reads1.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc.zip"
	ZIPfastqcResult_R2 = workdir + "/fastqc/" + reads2.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc.zip"

	#Full path to html files
	HTMLfastqcResult_R1 = workdir + "/fastqc/" + reads1.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc"
	HTMLfastqcResult_R2 = workdir + "/fastqc/" + reads2.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc"

	#inflate zip files
	os.system("unzip " + ZIPfastqcResult_R1 + " -d fastqc ")
	os.system("unzip " + ZIPfastqcResult_R2 + " -d fastqc ")

	listResult = [HTMLfastqcResult_R1, HTMLfastqcResult_R2, TXTfastqcResult_R1, TXTfastqcResult_R2]

	os.system('echo "\n************************** FASTQC REPORT : END **************************\n" >> '+ logFilepath)

	return listResult


def get_default_reference(genus,workdir,logFilepath):
	
	#DO NOT generate log for this function : it will be generated in its calling function

	if((os.path.exists(workdir + "/reference_iVARCAll"))==False):
		os.mkdir(workdir + "/reference_iVARCAll")

	if(genus == "Salmonella"):
		#retrieving reference filename
		reference_filename = "Typhimurium_LT2_AE006468.fasta" #need to be updated if reference path changes
		#unzip in current working directory with -d and junk paths with -j
		os.system("unzip -o -j /mnt/NAS/NASBIO1/DATA/REFERENCES/Salmonella/Typhimurium_LT2_AE006468.zip -d " + workdir + "/reference_iVARCAll") #need to be updated if reference path changes

	elif(genus == "Listeria"):
		#retrieving reference filename
		reference_filename = "Listeria_monocytogenes_strain_EGD-e.fasta" #need to be updated if reference path changes
		#unzip in current working directory with -d and junk paths with -j
		os.system("unzip -o -j /mnt/NAS/NASBIO1/DATA/REFERENCES/Listeria/EGD-e.zip -d " + workdir + "/reference_iVARCAll") #need to be updated if reference path changes

	elif(genus == "Staphylococcus"):
		#retrieving reference filename
		reference_filename = "Staphylococcus_aureus_NC_007795.fasta" #need to be updated if reference path changes
		#unzip in current working directory with -d and junk paths with -j
		os.system("unzip -o -j /mnt/NAS/NASBIO1/DATA/REFERENCES/Staphylococcus/Staphylococcus_aureus_NC_007795.zip -d " + workdir + "/reference_iVARCAll") #need to be updated if reference path changes
	
	else:	
		logFile = open(logFilepath, "a")
		logFile.write("ERROR: no reference for " + genus)
		logFile.close()
		shutil.rmtree(str(workdir + "/reference_iVARCAll"))
		sys.exit(1)

	
	if os.path.exists(workdir + "/reference_iVARCAll/" + reference_filename):
		return workdir + "/reference_iVARCAll/" + reference_filename

	#In case of unreachable reference(cant mount NAS,for example)
	else:
		logFile = open(logFilepath, "a")
		logFile.write("ERROR: unreachable reference for " + genus)
		logFile.close()
		shutil.rmtree(str(workdir + "/reference_iVARCAll"))
		sys.exit(1)



def commande_iVARCAll(genus, reads1, reads2, adaptaters, nbThreads, maxMemory, Gid, workdir,logFilepath, report):

    os.system('echo "\n************************** iVARCall2 WORKFLOW : START **************************\n" >> '+ logFilepath)

    logFile = open(logFilepath, "a")
    logFile.write("Get default reference...")
    reference = get_default_reference(genus,workdir,logFilepath)
    logFile.write("...ok\n")
    

    os.system('echo "iVARCall2 started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

    cmd="iVARCall2 -ref " + reference + " -reads " + reads1 + " " + reads2 + \
        " -a " + adaptaters + " -o " + Gid + "_iVARCAll -T " + str(nbThreads) + " -m " + str(maxMemory) + \
        " --removeDuplicates --indelRealigner --removeTmpFiles --onlyVCF" + " >> " + logFilepath + " 2>&1"
    
    logFile.write("\n" + cmd + "\n")
    logFile.close()
    
    os.system(cmd)

    os.system('echo "iVARCall2 ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

    iVARCAll_log = Gid + "_iVARCAll/" + Gid + "_iVARCAll.log" # récupérer le log créer par iVARCall
    VCF_file = Gid + "_iVARCAll/VCF/" + Gid + ".g.vcf" # récupérer path VCF

    iVARCall_log_parser(iVARCAll_log, VCF_file, reference, report)

    shutil.rmtree(Gid + "_iVARCAll/BAM") # remove BAM folder
    shutil.rmtree("reference_iVARCAll") # remove reference folder
    
    os.system('echo "\n************************** iVARCall2 WORKFLOW : END **************************\n" >> '+ logFilepath)

    return VCF_file
    

def iVARCall_log_parser(iVARCAll_log, VCF_file, reference, report):

	logFile = open(iVARCAll_log, "r")
	logLines = logFile.readlines()
	logFile.close()

	dicoStat = extractReadStatistics(logLines)

	report_file = open(report, "a")
	report_file.write(""+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+         iVARCALL 2 METRICS           +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n\n")

	reference = reference.split('/')[-1]
	report_file.write("REFERENCE GENOME : " + reference.split('.')[0] + '\n')
	report_file.write("BREADTH COVERAGE : " + dicoStat["breadth cov."] + '\n')
	report_file.write("TOTAL READS COUNT : " + dicoStat["reads"] + '\n')
	report_file.write("TOTAL READS COUNT AFTER TRIMMING : " + dicoStat["trimming"] + '\n')
	report_file.write("MAPPED READS : " + dicoStat["mapped"] + '\n')
	report_file.write("PAIRED MAPPED READS : " + dicoStat["paired"] + '\n')

	SNPnb = numberOfVariant_from_VCF(VCF_file)
	report_file.write("SNPs COUNT (UNFILTERED) : " + str(SNPnb) + '\n\n\n')

	report_file.close()


def extractReadStatistics(lines):
	"""
	Extract reads statistics from the log file lines.
	@param lines : lines of the log file 
	@type lines : list
	@return: dictionnary with sample name in key and for value an other \
	dictionnary with the number of reads before and after trimming, \
	the number of reads mapped and properly paired, \
	and the deep and breadth coverage.
	@rtype: dictionnary 
	"""

	dicoResult = {}

	for line in lines :
			
		if("Number of reads before trimming" in line):
			nbReads = line.split(' ')[6]
			nbReads = nbReads.rstrip()
			dicoResult["reads"] = nbReads

		if("(QC-passed reads + QC-failed reads)" in line):
			trimming = line.split(' ')[0]
			dicoResult["trimming"] = trimming	
			
		if(" mapped (" in line):
			mapped = line.split(' ')[0]
			dicoResult["mapped"] = mapped	

		if("properly paired" in line):
			paired = line.split(' ')[0]
			dicoResult["paired"] = paired
			
		if("Deep coverage" in line):
			deep = line.split(' ')[3]
			deep = deep.rstrip()
			dicoResult["deep cov."] = deep	

		if("Breadth coverage" in line):
			breadth = line.split(' ')[3]
			breadth = breadth.rstrip()
			dicoResult["breadth cov."] = breadth

	return dicoResult	


def numberOfVariant_from_VCF(VCF_file):

	vcfFile = open(VCF_file, "r")
	lines = vcfFile.readlines()
	vcfFile.close()

	SNPnb = 0

	for line in lines :
		if(line[0]!='#' and line.split('\t')[4]!='.'):
			SNPnb += 1

	return SNPnb		


def number_of_reads(reads_file):

	reads = open(reads_file, 'r')
	nbReads = len(reads.readlines())
	reads.close()

	return nbReads/4 #fastqfile = 4 lines per read


def MSLT_analysis(contigs, nbThreads, logFilepath, report, genus):
	
	os.system('echo "\n************************** MLST WORKFLOW : START **************************\n" >> '+ logFilepath)

	command = "mlst --quiet " + " --threads=" + str(nbThreads) + \
	" " + contigs + " > MLST_result.tmp"

	os.system(command)


	#result parser

	MLST_result = open("MLST_result.tmp", 'r')
	lines = MLST_result.readlines()
	MLST_result.close()

	RESULT_ST = lines[0].split('\t')[2]

	
	# write report
	
	report_file = open(report, "a")
	report_file.write("" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+              MLST RESULTS            +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n")	
	report_file.write("Genus+Sp" + "\t" + "CC or ST" + "\t" + "Genes" + "\n")
	report_file.write(str(list(lines[0].split('\t'))[1:])) 


	RESULT_MLST = [RESULT_ST]	

	logFile = open(logFilepath,"a")

	if(genus == "Salmonella"):
		logFile.write("get serovar from ST Salmonella...")
		RESULT_SEROVAR = get_serovar_from_ST_Salmonella(RESULT_ST)
		logFile.write("...ok")
		report_file.write("\nPREDICTED SEROVAR : " + RESULT_SEROVAR + "\n")
		RESULT_SEROVAR = RESULT_SEROVAR.split(' ')[0] # keep only serovar name
		RESULT_MLST.append(RESULT_SEROVAR)


	elif(genus == "Listeria"):
		logFile.write("get CC from ST Listeria...")
		RESULT_CC = get_CC_from_ST_Listeria(RESULT_ST)
		logFile.write("...ok")
		report_file.write("\nPREDICTED CLONAL COMPLEX : " + RESULT_CC + "\n\n\n")
		RESULT_MLST.append(RESULT_CC)

	logFile.close()
	report_file.close()

	os.system("cat MLST_result.tmp >> " + logFilepath)
	os.system("rm MLST_result.tmp")

	os.system('echo "*\n************************* MLST WORKFLOW : END **************************\n" >> '+ logFilepath)
	
	return RESULT_MLST


def get_CC_from_ST_Listeria(ST):

	tabFile = open("/mnt/NAS/NASBIO1/DATA/MLST_DATA/Listeria/profile.txt",'r')
	lines = tabFile.readlines()
	tabFile.close()

	for line in lines:

		line = line.split('\t')
		if(line[0] == ST and line[8] != ''):
			return line[8]

	return 'unknown'		


def get_serovar_from_ST_Salmonella(ST):

	tabFile = open("/mnt/NAS/NASBIO1/DATA/MLST_DATA/Salmonella/mlst_lookup.txt",'r')
	lines = tabFile.readlines()
	tabFile.close()

	dico_PredictedSerovar = {}

	for line in lines:

		line = line.split('\t')
		
		if(line[0] == ST):
			serovar = line[1].replace(' (Achtman)','').rstrip()
			serovar = serovar.replace(' (PHE)','').rstrip()

			if(serovar in dico_PredictedSerovar):
				dico_PredictedSerovar[serovar] += 1

			else:
				dico_PredictedSerovar[serovar] = 1	

	if 	len(dico_PredictedSerovar.keys()) == 0 :
		return 'unknown'		


	# Select best result with its score			

	max = 0
	predictedSerovar = ''
	scoreSerovar = 0

	for serovar in dico_PredictedSerovar :

		if(dico_PredictedSerovar[serovar] > scoreSerovar):
			predictedSerovar = serovar
			scoreSerovar = dico_PredictedSerovar[serovar]

		max = max + dico_PredictedSerovar[serovar]
		
	scorePourc = round(float(scoreSerovar)/max*100,2)
		
	predictedSerovar = predictedSerovar + " (" + str(scorePourc) + "%)"

	return predictedSerovar


def Assembly(reads1,reads2,cwd,logFilepath,nbThreads): #Perform de novo assembly with Spades

	os.system('echo "\n************************** DE NOVO ASSEMBLY WORKFLOW : START **************************\n" >> '+ logFilepath)
	AssemblyDeNovopath = cwd + "/Assembly"

	os.system('echo "Assembly started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	cmd = "spades --careful " + \
	" -1 " + reads1 + " " + \
	" -2 " + reads2 + " " + \
	" -o " + AssemblyDeNovopath + \
	" -t "  + str(nbThreads) + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	os.system('echo "Assembly ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	copyfile(AssemblyDeNovopath + "/contigs.fasta",cwd + "/contigs.fasta")
	copyfile(AssemblyDeNovopath + "/scaffolds.fasta",cwd + "/scaffolds.fasta")
	
	os.system('echo "\n************************** DE NOVO ASSEMBLY WORKFLOW : END  **************************\n" >> '+ logFilepath)

	#return contig file path
	return cwd + "/contigs.fasta"


def Assembly_control(AssemblyDeNovo, referencepath, MLST_id, minContig, GFF, cwd, logFilepath, nbThreads):

	os.system('echo "\n************************** ASSEMBLY CONTROL : START **************************\n" >> '+ logFilepath)
	
	quast_html_file = cwd + "/QUAST" 

	### Start Quast ###
	os.system('echo "Quast started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	cmd = "quast " + \
	" -R " + referencepath.rstrip() + \
	" -o " + quast_html_file + \
	" -t " + str(nbThreads) + ' ' + \
	" -m " + str(minContig) + " " +  \
	" -G " + GFF + " " +  \
	" --scaffolds " + \
	AssemblyDeNovo + \
	" >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	os.system('echo "Quast ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	os.system('echo "\n************************** ASSEMBLY CONTROL : END **************************\n" >> '+ logFilepath)

	return quast_html_file


def TrimContigsByLen(fasta_file, min_length,logFilepath): #In order to do not keep "bad" contigs that potentially arent a part of this genome

    os.system('echo "\n************************** CONTIGS TRIMMING : START **************************\n" >> '+ logFilepath)
    os.system('echo "Contigs trimming started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

    # Stock the sequences in a dictionnary
    sequences={}

    # Biopython fasta parsing
    for seq_record in SeqIO.parse(fasta_file, "fasta"):

        # Take the current sequence
        sequence = str(seq_record.seq).upper()

        # Keep sequence accordint to its length
        if (len(sequence) >= min_length):
            sequences[sequence] = seq_record.id

    # Write non-trimmed sequences == filtered output
    trimmedcontigs = open(fasta_file + "_trimmed.fasta", "w+")
    # Just read the hash table and write on the file as a fasta format
    for sequence in sequences:
            trimmedcontigs.write(">" + sequences[sequence] + "\n" + sequence + "\n")
    trimmedcontigs.close()
    
    os.system('echo "Contigs trimming ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath )
    os.system('echo "\n************************** CONTIGS TRIMMING : END **************************\n" >> '+ logFilepath)

    #return trimmed contigs file path
    return fasta_file + "_trimmed.fasta"


def scaffolding(contigs_file, referencepath, Gid, logFilepath ,maxMemory):

	os.system('echo "\n************************** SCAFFOLDING WORKFLOW : START **************************\n" >> '+ logFilepath)

	Maxmemoryg=Get_maxMemoryg(maxMemory) 
	scaffold_file = Gid + "_scaffold.fasta"
	
	os.system('echo "Scaffolding started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath )

	cmd = "java " + Maxmemoryg + " -jar /opt/medusa/1.0/medusa.jar -d -w2 -scriptPath /opt/medusa/1.0/medusa_scripts/ " + \
		" -i " + contigs_file + " -o " + scaffold_file + " -f " + referencepath + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)	

	os.system('echo "Scaffolding ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath )

	os.system('echo "\n************************** SCAFFOLDING WORKFLOW : END **************************\n" >> '+ logFilepath)


	return scaffold_file	


def get_scaffolding_reference(genus, MLST_id,logFilepath, report): 

	logFile = open(logFilepath, "a")
	logFile.write("Get scaffolding reference...")

	client = MongoClient('localhost', 27017)
	db = client.GAMeRdb
	reference = db.REFERENCE

	os.mkdir("reference_scaffolding")

	if(genus == "Salmonella"):
		if (reference.find({"Phylogeny.Serovar":MLST_id , "Phylogeny.Genus":genus}).count() > 0): 
			reference = reference.find({"Phylogeny.Serovar":MLST_id , "Phylogeny.Genus":genus})
			for element in reference :
				referenceZIP = element["ZIPpath"]
				break
			reference = referenceZIP	

		else :
			reference = "/mnt/NAS/NASBIO1/DATA/REFERENCES/Salmonella/Typhimurium_LT2_AE006468.zip"

	if(genus == "Listeria"):
		MLST_id = MLST_id[2:]
		if (reference.find({"Phylogeny.ClonalComplex":MLST_id , "Phylogeny.Genus":genus}).count() > 0): 

			reference = reference.find({"Phylogeny.ClonalComplex":MLST_id , "Phylogeny.Genus":genus})
			for element in reference :
				referenceZIP = element["ZIPpath"]
				break
			reference = referenceZIP	

		else :
			reference = "/mnt/NAS/NASBIO1/DATA/REFERENCES/Listeria/EGD-e.zip"


	if(genus == "Staphylococcus"):
		MLST_id = MLST_id[0]
		if (reference.find({"Phylogeny.SequenceType":MLST_id , "Phylogeny.Genus":genus}).count() > 0):		

			reference = reference.find({"Phylogeny.SequenceType":MLST_id , "Phylogeny.Genus":genus})
			for element in reference :
				referenceZIP = element["ZIPpath"]
				break
			reference = referenceZIP

		else :
			reference = "/mnt/NAS/NASBIO1/DATA/REFERENCES/Staphylococcus/Staphylococcus_aureus_NC_007795.zip"	


	os.system("unzip " + reference + ' -d reference_scaffolding/. "*.fasta"')		
	os.system("rm reference_scaffolding/*_prot.fasta")
			
	reference_fasta = "reference_scaffolding/"+str(subprocess.check_output("ls reference_scaffolding/",shell=True))
	fasta_ref_path= str(reference_fasta).rsplit('/',1)[0]+"/" # reference path for MeDuSa option -f

	logFile.write("...ok\n")
	logFile.write("...scaffolding reference : " + reference_fasta)
	logFile.close()

	os.mkdir("reference_gff")
	os.system("unzip " + reference + ' -d reference_gff/. "*.gff*"')
	reference_gff = "reference_gff/"+str(subprocess.check_output("ls reference_gff/",shell=True))


	report_file = open(report, "a")
	report_file.write("\nReference used for scaffolding : " + reference_fasta.split('/')[-1] + '\n')
	report_file.close()


	return reference_fasta,fasta_ref_path,reference_gff.rstrip()


def CloseGaps(scaffold, contigs, read1, read2, nbThreads, Gid, readsLen, logFilepath, report):

	os.system('echo "\n************************** CLOSING GAPS : START **************************\n" >> '+ logFilepath)

	#os.system("gunzip " + read1)
	#os.system("gunzip " + read2)

	#read1 = '.'.join(read1.split('.')[0:-1])
	#read2 = '.'.join(read2.split('.')[0:-1])

	os.system('echo "GMcloser started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)
	
	cmd = "gmcloser -t " + scaffold + " -q " + contigs + " -l " + str(readsLen) + \
	" -p gmclos -r " + read1 + " " + read2 + " -n " + str(nbThreads) + " -bq phred33 "

	cmd = cmd + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	os.system('echo "GMcloser ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	###################

	CloseGapsFile = Gid + "_scaffold_closed.fasta"

	if os.path.isfile("gmclos.gapclosed.fa"):
		#rename gapclosed output file
		os.system("mv gmclos.gapclosed.fa " + CloseGapsFile)

	else :
		os.system('echo "ERROR GMcloser no result"  ')	
		CloseGapsFile = scaffold
		rep = open(report, 'a')
		rep.write("\nERROR GMcloser --> step skipping\n")
		rep.close()


	###################

	os.system("rm -r gmclos*")	

	os.system('echo "\n************************** CLOSING GAPS : END **************************\n" >> '+ logFilepath)
	return CloseGapsFile


def rename_contig(fasta_file, genome_id,logFilepath):

	os.system('echo "\n************************** RENAME CONTIGS : START **************************\n" >> '+ logFilepath)

	fasta = open(fasta_file, 'r')
	fasta_lines = fasta.readlines()
	fasta.close()

	rename_fastaFile_name = genome_id + "_assembly.fasta"
	rename_fastaFile = open(rename_fastaFile_name, 'w')

	i = 1
	for line in fasta_lines :
		if line[0] == '>':
			rename_fastaFile.write('>' + genome_id + '_' + str(i) + '\n')
			i+=1
		else :
			j = 1
			for n in line :

				if j%70==0 :
					rename_fastaFile.write('\n')
			
				rename_fastaFile.write(n)
			
				j+=1	

	rename_fastaFile.close()

	os.system('echo "\n************************** RENAME CONTIGS : END **************************\n" >> '+ logFilepath)
	
	return rename_fastaFile_name	


def annotation(fasta_file, genome_id, genus, nbThreads, logFilepath):

	os.system('echo "\n************************** GENOME ANNOTATION : START **************************\n" >> '+ logFilepath)

	cmd = "prokka --quiet --genus " + genus + " --prefix annotation" + \
	" -cpus " + str(nbThreads) + " " + fasta_file
	cmd = cmd + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	annotation_gbk = genome_id + ".gbk"
	annotation_gff = genome_id + ".gff"

	os.system("mv annotation/annotation.gbk " + annotation_gbk)
	os.system("mv annotation/annotation.gff " + annotation_gff)

	os.system('echo "\n************************** GENOME ANNOTATION : END **************************\n" >> '+ logFilepath)

	return annotation_gbk, annotation_gff


def get_len(fastqc_txt):

    readslength = subprocess.check_output("grep 'Sequence length' " + fastqc_txt + " | cut -f2 ",shell=True).rstrip('\n')
    readslength = readslength.split('-')[-1]
    return readslength

	


#main function	
def main():	


	##########################################
	#			Initialisation				 #
	##########################################

	# Processing time counter
	t0 = time.time()
	
	# Get arguments 
	parser=get_parser()
	
	# Print parser.help if no arguments
	if len(sys.argv)==1:
		parser.print_help()
		sys.exit(1)
	
	Arguments=parser.parse_args()	

	print "Step 1/16 ---> Initialisation"

	# Go to working directory gived by Arguments.workdir
	if(Arguments.workdir != '.'):
		os.chdir(Arguments.workdir)	

	cwd = os.getcwd() #Current working directory FULL path	

	# GLOBAL VARIABLES
	GENOME_ID = '_'.join(Arguments.reads1.split('/')[-1].split('.')[0].split('_')[0:-1])
	LOG_FILE = cwd + "/" + GENOME_ID + ".log"
	REPORT_FILE = GENOME_ID + "_report.txt"
	FINAL_OUTPUT_DIRECTORY = "/mnt/NAS/NASBIO1/DATA/GAMeR_DB/" + Arguments.genus.upper() + '/' + GENOME_ID
	ADAPTATERS_FILE = "/mnt/NAS/NASBIO1/DATA/all_known_iontorrent-Illumina.fa"

	#Retrieve Python command
	command=""
	for arg in sys.argv:
		command = command + " " + arg	

	# First write in Log File : date and command
	LOG = open(LOG_FILE, 'w')
	LOG.write("READSPROCESS STARTED :" + time.strftime("%d/%m/%Y") + ' ' + time.strftime("%H:%M:%S") + '\n\n')
	LOG.write("COMMAND USED :" + command + '\n\n\n')
	LOG.close()

	# Check GENOME_ID
	check_finalDirectory_exist(FINAL_OUTPUT_DIRECTORY, LOG_FILE)
	check_MongoDB_exist(GENOME_ID, Arguments.genus, LOG_FILE)

	# Make final directory 
	os.makedirs(FINAL_OUTPUT_DIRECTORY)

	# First write in Report file : Metadata and key processing parameters
	REPORT = open(REPORT_FILE, 'w')
	REPORT.write("++++++++++++++++++++++++++++++++++++++++ DATASET REPORT  ++++++++++++++++++++++++++++++++++++++++\n" + \
	"GENERATED " + time.strftime("%d/%m/%Y") + ' AT ' + time.strftime("%H:%M:%S") + '\n' + \
	"READS : \n" + Arguments.reads1.split('/')[-1] + "\n" + Arguments.reads2.split('/')[-1] + '\n' + \
	"GENUS : " + Arguments.genus + '\n' + \
	"SP : " + Arguments.species + '\n' + \
	"\n"+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+                 METADATA             +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n\n" + \
	"Supplied by : " + Arguments.supplier + '\n' +\
	"Sequencing center : " + Arguments.center + '\n' + \
	"Sequencing technology : " + Arguments.technology + '\n' + \
	"Projet id : " + Arguments.projetID + '\n' + \
	"\n"+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+             KEY PARAMETERS           +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n"+ \
	"Coverage threshold : " + str(Arguments.minCov) + '\n' + \
	"Contig length threshold : " + str(Arguments.minContigLen) + '\n' + \
	"Phred score threshold : " + str(Arguments.minPhredScore) + '\n' + \
	"\n"+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+              MAIN METRICS            +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n")
	REPORT.close()


	##########################################
	#				Check reads				 #
	##########################################
	
	print "Step 2/16 ---> Check reads format"

	### READ 1 ###

	# If file isn't in the right format, analyse it and uncompress it if possible. 
	if(Arguments.reads1.split('.')[-1] not in ["fastq", "fq", "FASTQ", "FQ"]):
		Arguments.reads1,IS_corrupted = uncompress_in_workdir(Arguments.reads1, LOG_FILE,cwd)

		if (IS_corrupted==True):
			# ASTUCE : utiliser booleen IS_corrupted: En considerant que le script sera appelé pour chaque paire de reads, sauter le traitement du reads pair si le premier est déjà corrompu
			sys.exit(1) 

	# File is already in the right format : copy it in the current work directory		
	else:
		copyfile(Arguments.reads1, cwd + "/" + list(Arguments.reads1.rsplit('/',1))[1])
		Arguments.reads1= cwd + '/' + Arguments.reads1.split('/')[-1]
		LOG=open(LOG_FILE,"a")
		LOG.write("Reads already uncompressed in:" + Arguments.reads1 + "\n")
		LOG.close()


	### READ 2 : Same procedure (if read1 processed successfully, according to IS_corrupted ) ###


	if(Arguments.reads2.split('.')[-1] not in ["fastq", "fq", "FASTQ", "FQ"]):
		Arguments.reads2,IS_corrupted = uncompress_in_workdir(Arguments.reads2, LOG_FILE,cwd)
		
		if (IS_corrupted==True):
			sys.exit(1)

	else:
		copyfile(Arguments.reads2, cwd + "/" + list(Arguments.reads2.rsplit('/',1))[1])
		Arguments.reads2 = cwd + '/' + Arguments.reads2.split('/')[-1]	
		LOG=open(LOG_FILE,"a")
		LOG.write("reads already uncompressed in:" + Arguments.reads2)
		LOG.close()	


	##########################################
	#		    Coverage checking			 #
	##########################################
	
	print "Step 3/16 ---> quality control"
	coverage = Check_coverage(Arguments.reads1,Arguments.reads2,Arguments.genus,Arguments.minCov,Arguments.nbThreads,Arguments.maxMemory,cwd,LOG_FILE,REPORT_FILE)


	##########################################
	#		      Normalization 			 #
	##########################################
	
	print "Step 4/17 ---> normalization"
	if(coverage > Arguments.targetCov):
		reads1, reads2 = normalization(Arguments.reads1, Arguments.reads2, Arguments.targetCov, Arguments.nbThreads, GENOME_ID, LOG_FILE, REPORT_FILE)
		os.system("mv " + reads1 + " " + Arguments.reads1)
		os.system("mv " + reads2 + " " + Arguments.reads2)

	##########################################
	#		    	  FASTQC		    	 #
	##########################################

	print "Step 5/17 ---> FASTQC"

	fastqcResult = Fastqc(Arguments.reads1, Arguments.reads2, ADAPTATERS_FILE, "fastqc", Arguments.nbThreads, cwd, LOG_FILE, REPORT_FILE)

	FASTQC_R1 = fastqcResult[0]
	FASTQC_R2 = fastqcResult[1]
	fastqc_txtreport = fastqcResult[2]


	##########################################
	#		    	iVARCall		    	 #
	##########################################

	print "Step 6/17 ---> iVARCall"

	VCF_FILE = commande_iVARCAll(Arguments.genus, Arguments.reads1,Arguments.reads2, ADAPTATERS_FILE,\
		Arguments.nbThreads, Arguments.maxMemory, GENOME_ID,cwd, LOG_FILE, REPORT_FILE)


	##########################################
	#		    Trimming reads		    	 #
	##########################################

	print "Step 7/17 ---> Trimming"
	reads_trim =  soft_trimming(Arguments.reads1, Arguments.reads2, ADAPTATERS_FILE, Arguments.nbThreads, Arguments.maxMemory, Arguments.minPhredScore, GENOME_ID, cwd, REPORT_FILE, LOG_FILE)
	
	reads1_trim = reads_trim[0]
	reads2_trim = reads_trim[1]


	##########################################
	#	        De novo assembly        	 #
	##########################################	

	print "Step 8/17 ---> De novo assembly"

	CONTIGS = Assembly(reads1_trim,reads2_trim,cwd,LOG_FILE,Arguments.nbThreads)


	##########################################
	#	              MLST	    	    	 #
	##########################################

	print "Step 9/17 ---> MLST"

	MLST_result = MSLT_analysis(CONTIGS, Arguments.nbThreads, LOG_FILE, REPORT_FILE, Arguments.genus)
	RESULT_ST = MLST_result[0]

	if(Arguments.genus == "Salmonella"):
		RESULT_SEROVAR = MLST_result[1]
		MLST_id = RESULT_SEROVAR

	elif(Arguments.genus == "Listeria"):
		RESULT_CC = MLST_result[1]	
		MLST_id = RESULT_CC

	elif(Arguments.genus == "Staphylococcus"):
		MLST_id = RESULT_ST


	##########################################
	#	           Scaffolding   	         #
	##########################################

	print "Step 10/17 ---> Scaffolding"

	new_reference,new_reference_directory, GFFref = get_scaffolding_reference(Arguments.genus, MLST_id,LOG_FILE, REPORT_FILE)
	scaffold = scaffolding(CONTIGS, new_reference_directory, GENOME_ID, LOG_FILE, Arguments.maxMemory)


	##########################################
	#	          Gap Closing	             #
	##########################################	

	print "Step 11/17 ---> Gap closing"

	readsLen = get_len(str(fastqc_txtreport))
	scaffold = CloseGaps(scaffold, CONTIGS, reads1_trim, reads2_trim, Arguments.nbThreads, GENOME_ID, readsLen, LOG_FILE, REPORT_FILE)

	##########################################
	#	         Contig filtration     	 	 #
	##########################################

	print "Step 12/17 ---> Contig filtration"

	scaffold = TrimContigsByLen(scaffold, Arguments.minContigLen, LOG_FILE)


	##########################################
	#	      QUAST quality control     	 #
	##########################################

	print "Step 13/17 ---> Assembly control"

	QUAST = Assembly_control(scaffold, new_reference,MLST_id,Arguments.minContigLen,GFFref,cwd,LOG_FILE,Arguments.nbThreads)
	

	##########################################
	#	        Rename scaffolds           	 #
	##########################################

	print "Step 14/17 ---> Rename scaffold"

	ASSEMBLY = rename_contig(scaffold, GENOME_ID,LOG_FILE)


	##########################################
	#	        	Annotation            	 #
	##########################################

	print "Step 15/17 ---> Genome annotation"

	GBK, GFF = annotation(ASSEMBLY, GENOME_ID, Arguments.genus, Arguments.nbThreads, LOG_FILE)


	##########################################
	#	          Save results          	 #
	##########################################

	print "Step 16/17 ---> Moving files and add to mongoDB"

	sampleObjet = sample(GENOME_ID, Arguments.supplier, Arguments.projetID, REPORT_FILE, \
		Arguments.reads1, Arguments.reads2, readsLen, FASTQC_R1, FASTQC_R2, Arguments.center, \
		Arguments.technology, ASSEMBLY, CONTIGS, QUAST, GBK, GFF, Arguments.genus, \
		Arguments.species, RESULT_ST, MLST_id, VCF_FILE)
	sampleObjet.moveFiles(FINAL_OUTPUT_DIRECTORY)
	sampleObjet.insertMongo()


	##########################################
	#	        Remove tmp files	       	 #
	##########################################

	print "Step 17/17 ---> Cleaning temporary files"

	# Last write in log file : date and time elapsed
	t1 = time.time() - t0
	LOG = open(LOG_FILE, 'a')
	LOG.write(" \n\n\n READSPROCESS ENDED :" + time.strftime("%d/%m/%Y") + ' ' + time.strftime("%H:%M:%S") + "\n\n")
	LOG.write(" TIME ELAPSED (hour:min:sec) :" + str(datetime.timedelta(seconds=int(t1))))
	LOG.close()

	os.system("mv " + GENOME_ID + ".log " + FINAL_OUTPUT_DIRECTORY + "/.")

	# Remove tmp files and directories
	os.system("rm -r  " + GENOME_ID + "* temp reference_scaffolding Assembly \
		fastqc contigs.fasta* scaffolds.fasta stats annotation trashReads.fastq reference_gff")


if __name__ == "__main__":
	main()	        		
