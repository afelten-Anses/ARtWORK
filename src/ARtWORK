#!/usr/bin/env python
# -*- coding: iso-8859-1 -*-
import os, sys
import subprocess 
import datetime
import time
import argparse
import glob
from pyunpack import Archive 	
import shutil
import pymongo
from pymongo import MongoClient
from bson.objectid import ObjectId
from shutil import copyfile
import smtplib
from Bio import SeqIO
import patoolib
import re


__doc__="""
@requires: BWA
@requires: samtools
@requires: picard-tools
@requires: Trimmomatic
@requires: GATK
@requires: SnpSift
@requires: iVARCall2
@requires: pigz
@requires: tabix
@requires: vcf-merge
@requires: Vcf.pm (vcftools)
@requires: MongoDB
@requires: bbnorm
@requires: bbmap
@requires: fastqc
@requires: mlst
@requires: spades
@requires: quast
@requires: medusa
@requires: mash
@requires: gmcloser
@requires: prokka
@requires: seqsero
"""

def get_parser():
	"""
	Parse arguments
	@return: arguments list
	@rtype: parser object
	"""

	parser = argparse.ArgumentParser(description='run reads analysis, variant analysis, assembly and annotation \
		from raw reads and add outputs in the GAMeR database')

	parser.add_argument('-1', action="store", dest='reads1',
						type=str, required=True, help='Fastq file pair 1 with \'_R1\' (REQUIRED)')

	parser.add_argument('-2', action="store", dest='reads2',
						type=str, required=True, help='Fastq file pair 2 with \'_R2\' (REQUIRED)')

	parser.add_argument('-g', action="store", dest='genus',
						type=str, required=True, help='genus Salmonella/Listeria/Staphylococcus/Clostridium/Bacillus (REQUIRED)')

	parser.add_argument('-s', action="store", dest='species',
						type=str, required=True, help='species (REQUIRED)')

	parser.add_argument('-Mu', action="store", dest='MongoUser', 
						type=str, required=True, help='MongoDb username (REQUIRED)')

	parser.add_argument('-Mp', action="store", dest='MongoPassword', 
						type=str, required=True, help='MongoDb password (REQUIRED)')
	parser.add_argument('-Ms', action="store", dest='MongoServer',
                                                type=str, default="SAS-PP-LSCALC1", help='Server name or IP for GamerDB host server')

	parser.add_argument('-n', action="store", dest='NASBIO1_PATH',
						type=str, default="~/NASBIO1", help='NASBIO1/bio path (default:~/NASBIO1)')

	parser.add_argument('-minCov', action="store", dest='minCov',
						type=int, default=30, help='minimum coverage required for workflow (default:30)')

	parser.add_argument('-targetCov', action="store", dest='targetCov',
						type=int, default=100, help='minimum coverage required for normalization, 0 for no normalization (default:100)')

	parser.add_argument('-l', action="store", dest='minContigLen',
						type=int, default=200, help='minimum contig length (default:200)')

	parser.add_argument('-p', action="store", dest='minPhredScore',
						type=int, default=30, help='minimum phred score (default:30)')

	parser.add_argument('-T', action="store", dest='nbThreads', 
						type=int, default=1, help='maximum number of threads to use (default:1)')

	parser.add_argument('-m', action="store", dest='maxMemory', 
						type=int, default=4000, help='max memory to use in Mb (default:4000)')

	parser.add_argument('-w', action="store", dest='workdir', 
						type=str, default='.', help='working directory (default:current directory)')

	parser.add_argument('-a', action="store", dest='min_mapped_scaffold_percent', 
						type=int, default=80, help='minimum percent of total scaffold length \
						mapped on reference, used to detect contaminants (default:80)')

	parser.add_argument('-Ksize', action="store", dest='Ksize', 
						type=str, default='15', help='Kmer size for mash (default:15)')	

	parser.add_argument('-Ssize', action="store", dest='Ssize', 
						type=str, default='1000', help='Sketch size for mash (default:15)')										

	parser.add_argument('-d', action="store", dest='max_len_diff', 
						type=int, default=30, help='maximum percent of length difference \
						between assembly and reference (default:20)')

	parser.add_argument('-medusa', action="store", dest='medusa_path', type=str, help='medusa jar path ')
        
	parser.add_argument('-medusa_scripts', action="store", dest='medusa_scripts_path', type=str, help='medusa scripts path')

	parser.add_argument('-GATK', action="store", dest='GATK_path',type=str, help='GATK jar path')

	parser.add_argument('--supplier', action="store", dest='supplier', 
						type=str, default='Anses', help='Supplier (default:Anses)')

	parser.add_argument('--project', action="store", dest='projetID', 
						type=str, required=True, help='Project id')

	parser.add_argument('--center', action="store", dest='center', 
						type=str, required=True, help='sequencing center')

	parser.add_argument('--technology', action="store", dest='technology', 
						type=str, required=True, help='technology used for sequencing')


	return parser



class sample(object) :


	def __init__(self, SampleID, supplier, projetID, report, \
		reads1Path, reads2Path, readsLen, reads1QC, reads2QC, center, technology, \
		assembly, contigs, QUAST, GBK, GFF, \
		genus, species, ST, MLST_id, VCF, sketch, nbContigs, totalLength, \
		largestContig, n50, genomeFraction, refScafold, refSNP, breadthCoverage):
		"""
		Initialize the sample class
		"""

		self.id = SampleID	
		self.supplier = supplier
		self.projetID = projetID
		self.report = report

		self.nbReads = number_of_reads(reads1Path)
		self.reads1 = self.compressReads(reads1Path)
		self.reads2 = self.compressReads(reads2Path)
		self.readsLen = readsLen
		self.reads1QC = reads1QC
		self.reads2QC = reads2QC
		self.center = center
		self.technology = technology
		self.VCF = VCF
		self.RefSNP = refSNP
		self.BreadthCoverage = breadthCoverage

		self.assembly = assembly
		self.contigs = contigs
		self.QUAST = QUAST
		self.GBK = GBK
		self.GFF = GFF
		self.sketch = sketch
		self.NbContigs = nbContigs
		self.TotalLength = totalLength
		self.LargestContig = largestContig
		self.N50 = n50
		self.GenomeFraction = genomeFraction
		self.RefScafold = refScafold
		
		self.genus = genus
		self.species = species
		self.ST = ST

		if self.genus == "Salmonella" :
			self.serovar = MLST_id[0]
			self.antigenic_profile = MLST_id[1]

		elif self.genus == "Listeria" :
			self.CC = MLST_id[0] 	


	def moveFiles(self, FINAL_OUTPUT_DIRECTORY):

		mongo_output = '/'.join(FINAL_OUTPUT_DIRECTORY.split('/')[4:]) + '/'

		os.system("mv " + self.report + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")

		self.report = mongo_output + self.report.split("/")[-1]
		
		os.system("mv " + self.reads1 + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads1 = mongo_output + self.reads1.split("/")[-1]

		os.system("mv " + self.reads2 + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads2 = mongo_output + self.reads2.split("/")[-1]

		os.system("mv " + self.reads1QC + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads1QC = mongo_output + self.reads1QC.split("/")[-1]

		os.system("mv " + self.reads2QC + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads2QC = mongo_output + self.reads2QC.split("/")[-1]

		os.system("mv " + self.assembly + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.assembly = mongo_output + self.assembly.split("/")[-1]

		os.system("mv " + self.contigs + ' ' + FINAL_OUTPUT_DIRECTORY + "/" + self.id + "_contigs.fasta")
		self.contigs = mongo_output + self.id + "_contigs.fasta"

		os.system("mv " + self.QUAST + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.QUAST = mongo_output + self.QUAST.split("/")[-1]

		os.system("mv " + self.GBK + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.GBK = mongo_output + self.GBK.split("/")[-1]

		os.system("mv " + self.GFF + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.GFF = mongo_output + self.GFF.split("/")[-1]

		os.system("mv " + self.VCF + '* ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.VCF = mongo_output + self.VCF.split("/")[-1]

		os.system("mv " + self.sketch + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.sketch = mongo_output + self.sketch.split("/")[-1]


	def compressReads(self, reads):	

		if reads.split('.')[-1] != "gz" :
			os.system("pigz " + reads)
			reads = reads + ".gz"

		return reads	


	def insertMongo(self, MongoUser, MongoPassword, MongoServer):
		
		#client = MongoClient('localhost', username=MongoUser, \
		#	password=MongoPassword, authSource="GAMeRdb", authMechanism='SCRAM-SHA-1')
		uri = "mongodb://" + MongoUser + ":" + MongoPassword + "@"+MongoServer+"/GAMeRdb"
		client = MongoClient(uri)
		db = client.GAMeRdb
		genomes = db.GENOME

		document = {
			"SampleID":self.id,
			"Supplier":self.supplier,
			"Project":self.projetID,
			"Report":self.report,
			"Reads":{
				"FASTQ_pair1":self.reads1,
				"FASTQ_pair2":self.reads2,
				"NbReads":self.nbReads,
				"ReadsLength":self.readsLen,
				"FASTQC_pair1":self.reads1QC+"/fastqc_report.html",
				"FASTQC_pair2":self.reads2QC+"/fastqc_report.html",
				"Center":self.center,
				"Technology":self.technology,
				"VCF":self.VCF,
				"RefSNP":self.RefSNP,
				"BreadthCoverage":self.BreadthCoverage
			},
			"Genome":{
				"Assembly":self.assembly,
				"Contigs":self.contigs,
				"QUAST":self.QUAST+"/report.html",
				"GBK":self.GBK,
				"GFF":self.GFF,
				"Sketch":self.sketch,
				"NbContigs":self.NbContigs,
				"TotalLength":self.TotalLength,
				"LargestContig":self.LargestContig,
				"N50":self.N50,
				"GenomeFraction":self.GenomeFraction,
				"RefScaffold":self.RefScafold
			},
			"Phylogeny":{
				"Genus":self.genus,
				"Species":self.species
			}
		}

		if self.genus != "Clostridium" and self.genus != "Bacillus" :
			document["Phylogeny"]["SequenceType"] = self.ST

		if self.genus == "Salmonella" :
			document["Phylogeny"]["Serovar"] = self.serovar
			document["Phylogeny"]["Antigenic_profile"] = self.antigenic_profile

		elif self.genus == "Listeria" :	
			document["Phylogeny"]["ClonalComplex"] = self.CC	


		post_id = genomes.insert_one(document).inserted_id	


	def debug(self):	

		print("Genome id : " + self.id)
		print("Supplier : " + self.supplier)
		print("Projet id : " + self.projetID)
		print("Report path : " + self.report)

		print("Reads 1 path : " + self.reads1)
		print("Reads 2 path : " + self.reads2)
		print("Nb reads : " + str(self.nbReads))
		print("Read length : " + str(self.readsLen))
		print("Reads 1 path : " + self.reads1)
		print("FASTQC reads 1 path : " + self.reads1QC)
		print("FASTQC reads 2 path : " + self.reads2QC)
		print("Sequencing center : " + self.center)
		print("Sequencing technology : " + self.technology)
		print("VCF file : " + self.VCF)

		print("Assembly path : " + self.assembly)
		print("Contigs path : " + self.contigs)
		print("QUAST path : " + self.QUAST)
		print("GBK path : " + self.GBK)
		print("GFF path : " + self.GFF)
		print("Sketch path : " + self.sketch)

		print("Genus : " + self.genus)
		print("Species : " + self.species)
		print("ST : " + self.ST)

		if self.genus == "Salmonella" :
			print("Serovar : " + self.serovar)
			print("Antigenic profile : " + self.antigenic_profile)

		elif self.genus == "Listeria" :	
			print("Clonal Complex : " + self.CC)



def Get_maxMemoryg(maxMemory):

    #1go to 10go RAM conversion
    if(int(maxMemory) >= 1000 and int(maxMemory) < 10000):
        maxMemoryg = "-Xmx" + list(str(maxMemory))[0] + "g"

    #10go to 100go RAM conversion
    elif(int(maxMemory) >= 10000 and int(maxMemory) < 100000):
        maxMemoryg = "-Xmx" + str(list(str(maxMemory))[0] + list(str(maxMemory))[1]) + "g"

    #100go to 1000go RAM conversion
    elif(int(maxMemory) >= 100000 and int(maxMemory) < 1000000):
        maxMemoryg = "-Xmx" + str(list(str(maxMemory))[0] + list(str(maxMemory))[1] + list(str(maxMemory))[2]) + "g"

    #1To to 10To RAM conversion
    elif(int(maxMemory) >= 1000000 and int(maxMemory) < 10000000):
        maxMemoryg ="-Xmx" + str(list(str(maxMemory))[0] + list(str(maxMemory))[1] + list(str(maxMemory))[2] + list(str(maxMemory))[3]) + "g"

    #1go is the minimum requierement to be able to some tools, so if arguments.nbThreads<1go, enter Arguments.nbThreads converted defaut value
    else:
        maxMemoryg="-Xmx4g"
        
    return maxMemoryg
	

def check_finalDirectory_exist(FINAL_OUTPUT_DIRECTORY, logFilepath):
	
	logFile = open(logFilepath, "a")
	logFile.write("Check if final directory exists...")

	if(os.path.isdir(FINAL_OUTPUT_DIRECTORY) ==True):

		Gid = FINAL_OUTPUT_DIRECTORY.split('/')[-1]
		message = "ERROR:" + Gid + " already exist in " + FINAL_OUTPUT_DIRECTORY 

		#add msgerr in logFile
		logFile = open(logFilepath, 'a')
		logFile.write(message + '\n') 
		logFile.close()
		sys.exit(1)

	logFile.write("...ok\n")
	logFile.close()


def check_MongoDB_exist(GENOME_ID, genus, logFilepath, MongoUser, MongoPassword, MongoServer):
	
	logFile = open(logFilepath, "a")
	logFile.write("Check if genome already id exists in  database...")

	#client = MongoClient('localhost', username=MongoUser, \
	#		password=MongoPassword, authSource="GAMeRdb", authMechanism='SCRAM-SHA-1')
	uri = "mongodb://" + MongoUser + ":" + MongoPassword + "@"+MongoServer+"/GAMeRdb"
	client = MongoClient(uri)
	db = client.GAMeRdb
	genomes = db.GENOME

	if genomes.find({"SampleID":GENOME_ID , "Phylogeny.Genus":genus}).count() > 0:  #fix : if count > 0, genome exists in database

		message = "ERROR:" + GENOME_ID + " already exist in the database!!!"

		#add msgerr in logFile
		logFile = open(logFilepath, 'a') 
		logFile.write(message + '\n')
		logFile.close()
		sys.exit(1)

	logFile.write("...ok\n")
	logFile.close()


def uncompress_in_workdir(CompressedRead,logFilepath,workdir):


	################################################################
	## Extract reads_filename and file path from compressed file  ##
	################################################################

	os.system('echo "\n************************** READS UNPACKING : START **************************\n" >> '+ logFilepath)

	#File found
	if(os.path.exists(CompressedRead)):

		reads_filename = '.'.join(CompressedRead.split('.')[0:2])
		reads_filename = reads_filename.split('/')[-1]
		reads_filepath = workdir + '/' + reads_filename

	#File not found
	else:
		logFile = open(logFilepath, 'a')
		logFile.write("File not found : " + CompressedRead)
		logFile.close()
		sys.exit(1)
	

	##################################################
	##    Test and unpack archive if test is ok     ##
	##################################################

	### TEST ###

	os.system('echo "unpacking started at :" >> ' + logFilepath + ' 2>&1 | date >> ' + logFilepath + ' 2>&1')

	try:
		patoolib.test_archive(CompressedRead,verbosity=0)

	except:
		logFile = open(logFilepath, 'a')
		logFile.write("Problem with file : " + CompressedRead)
		os.system('echo "unpacking stopped with status error 1 at" : >> ' + logFilepath + ' 2>&1 | date >> ' + logFilepath + ' 2>&1')
		is_corruputed=True
		sys.exit(1)

	
	### IF TEST=SUCESS --> UNPACK IN CURRENT WORKING DIRECTORY ###

	else:
		Archive(CompressedRead).extractall(workdir)
		is_corruputed=False

	os.system('echo "unpacking finished sucessfully at" : >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

	os.system('echo "\n************************** READS UNPACKING : END **************************\n" >> '+ logFilepath)

	return reads_filename,is_corruputed


def normalization(reads1, reads2, targetCov, nbThreads, Gid, logFilepath, reportFilepath):

	reads1_normalized = Gid + "_norm_R1.fastq"
	reads2_normalized = Gid + "_norm_R2.fastq"

	cmd = "bbnorm.sh -Xmx20g " + \
	"in=" + reads1 + " " + \
	"in2=" + reads2 + " " + \
	"target=" + str(targetCov) + " " + \
	"mindepth=-1 " + \
	"maxdepth=1 " + \
	"out=" + reads1_normalized + " " + \
	"out2=" + reads2_normalized + " " + \
	"outt=trashReads.fastq " + \
	"threads=" + str(nbThreads) + " " + \
	"prefilter=t " + \
	"tossbadreads=t" 

	logFile = open(logFilepath, "a")
	logFile.write("\nReads Normalization to " + str(targetCov) + "X\n")
	logFile.write(cmd + "\n")
	logFile.close()

	os.system('echo "bbnorm started at : " >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)
	os.system(cmd)
	os.system('echo "bbnorm ended at" : >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

	report_file = open(reportFilepath, "a")
	report_file.write("TARGET COVERAGE FOR NORMALIZATION : " + str(targetCov) + 'X\n\n')
	report_file.close()
	
	return reads1_normalized, reads2_normalized
    

def Check_coverage(reads1, reads2, genus, minCov, nbThreads, maxMemory, workdir, logFilepath, reportFilepath, NASBIO1_path):
    
    os.system('echo "\n************************** QUALITY CONTROL (Check coverage) : START **************************\n" >> '+ logFilepath)

    #Convert maxMemory argument : Aaaa int to to -XmxAg form
    Maxmemoryg=Get_maxMemoryg(maxMemory)

    logFile = open(logFilepath, "a")
    logFile.write("Retrieving default reference...")

    #Get default reference before mapping accorded to this reference
    reference=get_default_reference(genus,workdir,logFilepath, NASBIO1_path, reads1, reads2, nbThreads)

    logFile.write("...ok\n")

    os.system('echo "bbmap started at : " >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

    ##################################################
    ## Map the reads and generate its quality stats ##
    ##################################################
    cmd = "bbmap.sh " +\
    "in="+reads1+" "+\
    "in2="+reads2+" "+\
    "ref="+reference+" "+\
    "covstats=" + workdir +"/stats/covstats.txt "+\
    "covhist=" + workdir + "/stats/covhist.txt "+\
    "bincov=" + workdir + "/stats/bincov.txt "+\
    "rpkm=" + workdir + "/stats/rpkm_fpkm.txt "+\
    "ehist=" + workdir + "/stats/error_hist "+\
    "statsfile=" + workdir + "/stats/reads_summary.txt "+\
    "covbinsize=1000 " + \
    "k=13 "+\
    "threads="+str(nbThreads)+" "+\
    "nodisk "+\
    str(Maxmemoryg)+" "+\
    ">> " + str(logFilepath) + " 2>&1"

    logFile.write("\n" + cmd + "\n")
    os.system(cmd)

    os.system('echo "bbmap ended at" : >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

    #####################
    ## Quality control ##
    #####################
    logFile.write("quality control : check medium coverage")
    #si coverage < minCov ---> écrire message d'erreur dans stdin + report + os.exit(1)
    #ajouter coverage trouvé + std variantion dans report
    coverage=subprocess.check_output("cat " + workdir + "/stats/covstats.txt | cut -f2 | sed -n \"2 p\"", shell=True).rstrip().replace(',','.')
    coverage_std_var=subprocess.check_output("cat " + workdir + "/stats/covstats.txt | cut -f11 | sed -n \"2 p\"", shell=True)
    
    report_file = open(reportFilepath, "a")
    report_file.write("READS DEEP COVERAGE : " + coverage + '\n')
    report_file.write("COVERAGE STANDARD DEVIATION : " + coverage_std_var + "\n")
    report_file.close()

    if (float(coverage) < float(minCov)):
        logFile.write("\n Warning, too low coverage : " + coverage + "x")
        logFile.close()
        sys.exit(1)

    logFile.write("...ok\n")
    logFile.write("RPKM / FPKM : \n")
    os.system("cat stats/rpkm_fpkm.txt >> " + logFilepath)
    logFile.write("\nCoverage histogram : \n")
    os.system("cat stats/covhist.txt >> " + logFilepath)
    logFile.close()

    os.system('echo "\n************************** QUALITY CONTROL (Check coverage) : END **************************\n" >> '+ logFilepath)

    return int(float(coverage))


def soft_trimming(reads1, reads2, adaptaters, nbThreads, maxMemory, phredscore, Gid, workdir, report, logFilepath):
	
	os.system('echo "************************** TRIMMING WORKFLOW : START ************************** " >> '+ logFilepath)

	Maxmemoryg=Get_maxMemoryg(maxMemory)
	logFile = open(logFilepath, "a")

	##################################################
	## Map the reads and generate its quality stats ##
	##################################################

	logFile.write("Calculating number of reads per file...")
	nbReads_before_trimming = number_of_reads(reads1)
	logFile.write("...ok\n")

	forward_paired = workdir + "/" + Gid + "_paired_R1.fq"
	forward_unpaired = workdir + "/" + Gid + "_unpaired_R1.fq.gz"
	reverse_paired = workdir + "/" + Gid + "_paired_R2.fq"
	reverse_unpaired = workdir + "/" + Gid + "_unpaired_R2.fq.gz"

	os.system('echo "Trimmomatic started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	command = "trimmomatic " + str(Maxmemoryg) + " PE -threads " + \
	str(nbThreads) + " -phred33 " + reads1 + " " + reads2 + " " + forward_paired + " " + \
	forward_unpaired + " " + reverse_paired + " " + reverse_unpaired + \
	" ILLUMINACLIP:" + adaptaters + ":2:" + str(phredscore) + ":15 " + \
	"TRAILING:20 MINLEN:50 >> " + logFilepath + " 2>&1"

	logFile.write("\n" + command + "\n")
	os.system(command)
	os.system('echo "Trimmomatic ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	#os.system("pigz " + forward_paired + ".gz")
	#os.system("pigz " + reverse_paired + ".gz")

	os.remove(forward_unpaired)
	os.remove(reverse_unpaired)

	logFile.write("Calculating number of reads per file after trimming...")
	nbReads_after_trimming = number_of_reads(forward_paired)
	logFile.write("...ok\n")
	logFile.close()

	report_file = open(report, "a")
	report_file.write("" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+            TRIMMING RESULTS           +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n")
	report_file.write("READS TRIMMING PARAMETERS : minQual = 20 -- minLen = 50 -- minPhred = " + str(phredscore) + "\n")
	report_file.write("READS COUNT BY FILE BEFORE TRIMMING : " + str(nbReads_before_trimming) + " \n")
	report_file.write("READS COUNT BY FILE AFTER TRIMMING : " + str(nbReads_after_trimming) + " \n")
	report_file.write("TRIMMED READS COUNT : " + str(int(nbReads_before_trimming-nbReads_after_trimming)) + "\n\n")
	report_file.close()

	list_reads_trim = [forward_paired, reverse_paired]
	
	os.system('echo "\n************************** TRIMMING WORKFLOW : END **************************\n" >> '+ logFilepath)
	return list_reads_trim


def Fastqc(reads1, reads2, adaptators, outputName, nbThreads, workdir, logFilepath, reportFilepath):

	#A mettre dans fichier REPORT : lien vers le report html

	os.system('echo "\n************************** FASTQC REPORT : START **************************\n" >> '+ logFilepath)

	os.mkdir(workdir + "/" + outputName)
	os.system('echo "FastQC started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	cmd = "fastqc -q -t " + str(nbThreads) + " --contaminants " + adaptators + \
		" -o " + workdir + "/" + outputName + " " + reads1 + " " + reads2 + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	os.system(cmd)

	os.system('echo "FastQC ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	#Full path to zip files
	TXTfastqcResult_R1 = workdir + "/fastqc/" + reads1.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc/fastqc_data.txt"
	TXTfastqcResult_R2 = workdir + "/fastqc/" + reads2.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc/fastqc_data.txt"

	#Full path to txt files
	ZIPfastqcResult_R1 = workdir + "/fastqc/" + reads1.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc.zip"
	ZIPfastqcResult_R2 = workdir + "/fastqc/" + reads2.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc.zip"

	#Full path to html files
	HTMLfastqcResult_R1 = workdir + "/fastqc/" + reads1.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc"
	HTMLfastqcResult_R2 = workdir + "/fastqc/" + reads2.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc"

	#inflate zip files
	os.system("unzip " + ZIPfastqcResult_R1 + " -d fastqc ")
	os.system("unzip " + ZIPfastqcResult_R2 + " -d fastqc ")

	listResult = [HTMLfastqcResult_R1, HTMLfastqcResult_R2, TXTfastqcResult_R1, TXTfastqcResult_R2]

	os.system('echo "\n************************** FASTQC REPORT : END **************************\n" >> '+ logFilepath)

	return listResult


def get_default_reference(genus, workdir, logFilepath, NASBIO1_path, reads1, reads2, nbThreads):
	
	#DO NOT generate log for this function : it will be generated in its calling function

	if((os.path.exists(workdir + "/reference_iVARCAll"))==False):
		os.mkdir(workdir + "/reference_iVARCAll")

	if(genus == "Salmonella"):
		#retrieving reference filename
		reference_filename = "Typhimurium_LT2_AE006468.fasta" #need to be updated if reference path changes
		#unzip in current working directory with -d and junk paths with -j
		os.system("unzip -o -j " + NASBIO1_path + "/DATA/REFERENCES/Salmonella/Typhimurium_LT2_AE006468.zip -d " + workdir + "/reference_iVARCAll") #need to be updated if reference path changes

	elif(genus == "Listeria"):
		#retrieving reference filename
		reference_filename = "Listeria_monocytogenes_strain_EGD-e.fasta" #need to be updated if reference path changes
		#unzip in current working directory with -d and junk paths with -j
		os.system("unzip -o -j " + NASBIO1_path + "/DATA/REFERENCES/Listeria/EGD-e.zip -d " + workdir + "/reference_iVARCAll") #need to be updated if reference path changes

	elif(genus == "Staphylococcus"):
		#retrieving reference filename
		reference_filename = "Staphylococcus_aureus_NC_007795.fasta" #need to be updated if reference path changes
		#unzip in current working directory with -d and junk paths with -j
		os.system("unzip -o -j " + NASBIO1_path + "/DATA/REFERENCES/Staphylococcus/Staphylococcus_aureus_NC_007795.zip -d " + workdir + "/reference_iVARCAll") #need to be updated if reference path changes
	
	elif(genus == "Clostridium"):
		#retrieving reference filename
		reference_filename = "CP000246.fasta" #need to be updated if reference path changes
		#unzip in current working directory with -d and junk paths with -j
		os.system("unzip -o -j " + NASBIO1_path + "/DATA/REFERENCES/Clostridium/CP000246.zip -d " + workdir + "/reference_iVARCAll") #need to be updated if reference path changes	

	elif(genus == "Bacillus"):
		#retrieving reference filename
		os.mkdir(workdir + "/genomes_Bacillus")
		os.system("cp " + NASBIO1_path + "/DATA/REFERENCES/Bacillus/*zip " + workdir + "/genomes_Bacillus/.")
		os.system("unzip \'" + workdir + "/genomes_Bacillus/*.zip\' \'*.fasta\' -d genomes_Bacillus/")
		os.system("rm " + workdir + "/genomes_Bacillus/*_prot.fasta " + workdir + "/genomes_Bacillus/*zip")

		list_FASTA = os.listdir(workdir + "/genomes_Bacillus/")
		listdir = (" " + workdir + "/genomes_Bacillus/").join(list_FASTA)
		
		os.system("mash sketch -o " + workdir + "/genomes_Bacillus/references " + workdir + "/genomes_Bacillus/" + listdir)
		os.system("mash sketch -r -o " + workdir + "/genomes_Bacillus/reads " + reads1 + " " + reads2)
		os.system("mash dist -p " + str(nbThreads) + " " + workdir + "/genomes_Bacillus/references.msh " + workdir + \
		"/genomes_Bacillus/reads.msh " + " > " + workdir + "/genomes_Bacillus/mash_result.tsv")

		os.system("cat " + workdir + "/genomes_Bacillus/mash_result.tsv >> " + logFilepath)
		mash_resultFile = open(workdir + "/genomes_Bacillus/mash_result.tsv",'r')

		lines = mash_resultFile.readlines()
		mash_resultFile.close()
		genome_ref = ""
		minDist = 1.0

		for line in lines :
			line = line.rstrip().split('\t')
			genome = line[0]
			dist = float(line[2])
			if dist < minDist :
				minDist = dist
				genome_ref = genome.split('/')[-1]

		os.system("cp " + workdir + "/genomes_Bacillus/" + genome_ref + " " + workdir + "/reference_iVARCAll")	
		reference_filename = genome_ref	
		os.system("rm -r " + workdir + "/genomes_Bacillus")

	else:	
		logFile = open(logFilepath, "a")
		logFile.write("ERROR: no reference for " + genus + "!")
		logFile.close()
		shutil.rmtree(str(workdir + "/reference_iVARCAll"))
		sys.exit(1)

	
	if os.path.exists(workdir + "/reference_iVARCAll/" + reference_filename):
		return workdir + "/reference_iVARCAll/" + reference_filename

	#In case of unreachable reference(cant mount NAS,for example)
	else:
		logFile = open(logFilepath, "a")
		logFile.write("ERROR: unreachable reference for " + genus)
		logFile.close()
		shutil.rmtree(str(workdir + "/reference_iVARCAll"))
		sys.exit(1)


def commande_iVARCAll(genus, reads1, reads2, adaptaters, nbThreads, maxMemory, Gid, workdir, logFilepath, report, NASBIO1_path, GATK_path):

    os.system('echo "\n************************** iVARCall2 WORKFLOW : START **************************\n" >> '+ logFilepath)

    logFile = open(logFilepath, "a")
    logFile.write("Get default reference...")
    reference = get_default_reference(genus,workdir,logFilepath, NASBIO1_path, reads1, reads2, nbThreads)
    logFile.write("...ok\n")
    

    os.system('echo "iVARCall2 started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

    cmd="iVARCall2 -ref " + reference + " -reads " + reads1 + " " + reads2 + \
        " -a " + adaptaters + " -o " + Gid + "_iVARCAll -T " + str(nbThreads) + " -m " + str(maxMemory) + \
	" -GATKJAR " + str(GATK_path) + \
        " --removeDuplicates --indelRealigner --removeTmpFiles --onlyVCF" + " >> " + logFilepath + " 2>&1"
    #cmd="iVARCall2 -ref " + reference + " -reads " + reads1 + " " + reads2 + \
    #    " -a " + adaptaters + " -o " + Gid + "_iVARCAll -T " + str(nbThreads) + " -m " + str(maxMemory) + \
    #            " -TRIMJAR " + trimmomatic_path + " -GATKJAR " + GATK_path + " -PICARDJAR " + picard_path + \
    #    " --removeDuplicates --indelRealigner --removeTmpFiles --onlyVCF" + " >> " + logFilepath + " 2>&1"

    logFile.write("\n" + cmd + "\n")
    logFile.close()
    
    os.system(cmd)

    os.system('echo "iVARCall2 ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

    iVARCAll_log = Gid + "_iVARCAll/" + Gid + "_iVARCAll.log" # récupérer le log créer par iVARCall
    VCF_file = Gid + "_iVARCAll/VCF/" + Gid + ".g.vcf" # récupérer path VCF

    iVARCall_log_parser(iVARCAll_log, VCF_file, reference, report)

    shutil.rmtree(Gid + "_iVARCAll/BAM") # remove BAM folder
    shutil.rmtree("reference_iVARCAll") # remove reference folder
    
    os.system('echo "\n************************** iVARCall2 WORKFLOW : END **************************\n" >> '+ logFilepath)

    return VCF_file
    

def iVARCall_log_parser(iVARCAll_log, VCF_file, reference, report):

	logFile = open(iVARCAll_log, "r")
	logLines = logFile.readlines()
	logFile.close()

	dicoStat = extractReadStatistics(logLines)

	report_file = open(report, "a")
	report_file.write(""+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+         iVARCALL 2 METRICS           +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n\n")

	reference = reference.split('/')[-1]
	report_file.write("REFERENCE GENOME : " + reference.split('.')[0] + '\n')
	report_file.write("BREADTH COVERAGE : " + dicoStat["breadth cov."] + '\n')
	report_file.write("TOTAL READS COUNT : " + dicoStat["reads"] + '\n')
	report_file.write("TOTAL READS COUNT AFTER TRIMMING : " + dicoStat["trimming"] + '\n')
	report_file.write("MAPPED READS : " + dicoStat["mapped"] + '\n')
	report_file.write("PAIRED MAPPED READS : " + dicoStat["paired"] + '\n')

	SNPnb = numberOfVariant_from_VCF(VCF_file)
	report_file.write("SNPs COUNT (UNFILTERED) : " + str(SNPnb) + '\n\n\n')

	report_file.close()


def extractReadStatistics(lines):
	"""
	Extract reads statistics from the log file lines.
	@param lines : lines of the log file 
	@type lines : list
	@return: dictionnary with sample name in key and for value an other \
	dictionnary with the number of reads before and after trimming, \
	the number of reads mapped and properly paired, \
	and the deep and breadth coverage.
	@rtype: dictionnary 
	"""

	dicoResult = {}

	for line in lines :
			
		if("Number of reads before trimming" in line):
			nbReads = line.split(' ')[6]
			nbReads = nbReads.rstrip()
			dicoResult["reads"] = nbReads

		if("(QC-passed reads + QC-failed reads)" in line):
			trimming = line.split(' ')[0]
			dicoResult["trimming"] = trimming	
			
		if(" mapped (" in line):
			mapped = line.split(' ')[0]
			dicoResult["mapped"] = mapped	

		if("properly paired" in line):
			paired = line.split(' ')[0]
			dicoResult["paired"] = paired
			
		if("Deep coverage" in line):
			deep = line.split(' ')[3]
			deep = deep.rstrip()
			dicoResult["deep cov."] = deep	

		if("Breadth coverage" in line):
			breadth = line.split(' ')[3]
			breadth = breadth.rstrip()
			dicoResult["breadth cov."] = breadth

	return dicoResult	


def numberOfVariant_from_VCF(VCF_file):

	vcfFile = open(VCF_file, "r")
	lines = vcfFile.readlines()
	vcfFile.close()

	SNPnb = 0

	for line in lines :
		if(line[0]!='#' and line.split('\t')[4]!='.'):
			SNPnb += 1

	return SNPnb		


def number_of_reads(reads_file):

	reads = open(reads_file, 'r')
	nbReads = len(reads.readlines())
	reads.close()

	return nbReads/4 #fastqfile = 4 lines per read


def MSLT_analysis(contigs, nbThreads, logFilepath, report, genus, NASBIO1_path, cwd):
	
	os.system('echo "\n************************** MLST WORKFLOW : START **************************\n" >> '+ logFilepath)

	command = "mlst --quiet " + " --threads=" + str(nbThreads) + \
	" " + contigs + " > MLST_result.tmp"

	os.system(command)


	#result parser

	MLST_result = open("MLST_result.tmp", 'r')
	lines = MLST_result.readlines()
	MLST_result.close()

	RESULT_ST = lines[0].split('\t')[2]

	
	# write report
	
	report_file = open(report, "a")
	report_file.write("" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+              MLST RESULTS            +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n")	
	report_file.write("Genus+Sp" + "\t" + "CC or ST" + "\t" + "Genes" + "\n")
	report_file.write(str(list(lines[0].split('\t'))[1:])) 


	RESULT_MLST = [RESULT_ST]	

	logFile = open(logFilepath,"a")

	if(genus == "Salmonella"):
		print("get serovar from ST Salmonella after scafolding step")
		#logFile.write("get serovar from ST Salmonella...")
		#RESULT_SEROVAR, RESULT_AntigenicProfile = get_serovar_from_ST_Salmonella(contigs, logFilepath, report, cwd)
		#logFile.write("...ok")
		#report_file.write("\nPREDICTED SEROVAR : " + RESULT_SEROVAR + "\n")
		#report_file.write("\nPREDICTED ANTIGENIC PROFILE : " + RESULT_AntigenicProfile + "\n")
		#RESULT_MLST.append(RESULT_SEROVAR)
		#RESULT_MLST.append(RESULT_AntigenicProfile)


	elif(genus == "Listeria"):
		logFile.write("get CC from ST Listeria...")
		RESULT_CC = get_CC_from_ST_Listeria(RESULT_ST, NASBIO1_path)
		logFile.write("...ok")
		report_file.write("\nPREDICTED CLONAL COMPLEX : " + RESULT_CC + "\n\n\n")
		RESULT_MLST.append(RESULT_CC)

	logFile.close()
	report_file.close()

	os.system("cat MLST_result.tmp >> " + logFilepath)
	os.system("rm MLST_result.tmp")

	os.system('echo "*\n************************* MLST WORKFLOW : END **************************\n" >> '+ logFilepath)
	
	return RESULT_MLST


def get_CC_from_ST_Listeria(ST, NASBIO1_path):

	tabFile = open(NASBIO1_path + "/DATA/MLST_DATA/Listeria/profile.txt",'r')
	lines = tabFile.readlines()
	tabFile.close()

	for line in lines:

		line = line.split('\t')
		if(line[0] == ST and line[8] != ''):
			return line[8]

	return 'unknown'		

''' DEPRECIATED
def get_serovar_from_ST_Salmonella(ST, NASBIO1_path):

	tabFile = open(NASBIO1_path + "/DATA/MLST_DATA/Salmonella/mlst_lookup.txt",'r')
	lines = tabFile.readlines()
	tabFile.close()

	dico_PredictedSerovar = {}

	for line in lines:

		line = line.split('\t')
		
		if(line[0] == ST):
			serovar = line[1].replace(' (Achtman)','').rstrip()
			serovar = serovar.replace(' (PHE)','').rstrip()

			if(serovar in dico_PredictedSerovar):
				dico_PredictedSerovar[serovar] += 1

			else:
				dico_PredictedSerovar[serovar] = 1	

	if 	len(dico_PredictedSerovar.keys()) == 0 :
		return 'unknown'		


	# Select best result with its score			

	max = 0
	predictedSerovar = ''
	scoreSerovar = 0

	for serovar in dico_PredictedSerovar :

		if(dico_PredictedSerovar[serovar] > scoreSerovar):
			predictedSerovar = serovar
			scoreSerovar = dico_PredictedSerovar[serovar]

		max = max + dico_PredictedSerovar[serovar]
		
	scorePourc = round(float(scoreSerovar)/max*100,2)
		
	predictedSerovar = predictedSerovar + " (" + str(scorePourc) + "%)"

	return predictedSerovar
'''

def get_serovar_from_ST_Salmonella(contigs,logFilepath, report, cwd):

	output = cwd + "/SeqSero_ouptut"
	cmd = "SeqSero.py -m 4 -i " + contigs + " -b mem -d " + output + " >> " + logFilepath
	os.system(cmd)
	
	#Parse SeqSero output
	SeqSero_file = open(output + "/Seqsero_result.txt", 'r')
	lines = SeqSero_file.readlines()
	try:
		predictedSerovar = lines[5].rstrip().split('\t')[1]
		predictedSerovar = re.sub('\(.*','',predictedSerovar)
		predictedSerovar = predictedSerovar.replace('*','')
		if predictedSerovar == "potential monophasic variant of Typhimurium":
			predictedSerovar = "Typhimurium monophasic variant"
		elif ' or ' in predictedSerovar or 'potential ' in predictedSerovar :
			predictedSerovar = "N/A"
	except:
		predictedSerovar = "N/A"
		
	if "N/A" in predictedSerovar:
		predictedSerovar = "N/A"
		predictedAntigenicProfile = "N/A"

	else :
		predictedAntigenicProfile = lines[4].rstrip().split('\t')[1]	
				
	shutil.rmtree(output)

	report_file = open(report, 'a')
	report_file.write("\nPREDICTED SEROVAR : " + predictedSerovar + "\n")
	report_file.write("\nPREDICTED ANTIGENIC PROFILE : " + predictedAntigenicProfile + "\n")
	report_file.close()	
	
	return predictedSerovar, predictedAntigenicProfile


def Assembly(reads1,reads2,cwd,logFilepath,nbThreads): #Perform de novo assembly with Spades

	os.system('echo "\n************************** DE NOVO ASSEMBLY WORKFLOW : START **************************\n" >> '+ logFilepath)
	AssemblyDeNovopath = cwd + "/Assembly"

	os.system('echo "Assembly started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	cmd = "spades.py --careful " + \
	" -1 " + reads1 + " " + \
	" -2 " + reads2 + " " + \
	" -o " + AssemblyDeNovopath + \
	" -t "  + str(nbThreads) + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	os.system('echo "Assembly ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	copyfile(AssemblyDeNovopath + "/contigs.fasta",cwd + "/contigs.fasta")
	copyfile(AssemblyDeNovopath + "/scaffolds.fasta",cwd + "/scaffolds.fasta")
	
	os.system('echo "\n************************** DE NOVO ASSEMBLY WORKFLOW : END  **************************\n" >> '+ logFilepath)

	#return contig file path
	return cwd + "/contigs.fasta"


def Assembly_control(AssemblyDeNovo, referencepath, minContig, GFF, cwd, logFilepath, nbThreads):

	os.system('echo "\n************************** ASSEMBLY CONTROL : START **************************\n" >> '+ logFilepath)
	
	quast_html_file = cwd + "/QUAST" 

	### Start Quast ###
	os.system('echo "Quast started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	cmd = "quast " + \
	" -R " + referencepath.rstrip() + \
	" -o " + quast_html_file + \
	" -t " + str(nbThreads) + ' ' + \
	" -m " + str(minContig) + " " +  \
	" -G " + GFF + " " +  \
	" --split-scaffolds " + \
	AssemblyDeNovo + \
	" >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	os.system('echo "Quast ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	os.system('echo "\n************************** ASSEMBLY CONTROL : END **************************\n" >> '+ logFilepath)

	return quast_html_file


def TrimContigsByLen(fasta_file, min_length,logFilepath): #In order to do not keep "bad" contigs that potentially arent a part of this genome

    os.system('echo "\n************************** CONTIGS TRIMMING : START **************************\n" >> '+ logFilepath)
    os.system('echo "Contigs trimming started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

    # Stock the sequences in a dictionnary
    sequences={}

    # Biopython fasta parsing
    for seq_record in SeqIO.parse(fasta_file, "fasta"):

        # Take the current sequence
        sequence = str(seq_record.seq).upper()

        # Keep sequence accordint to its length
        if (len(sequence) >= min_length):
            sequences[sequence] = seq_record.id

    # Write non-trimmed sequences == filtered output
    trimmedcontigs = open(fasta_file + "_trimmed.fasta", "w+")
    # Just read the hash table and write on the file as a fasta format
    for sequence in sequences:
            trimmedcontigs.write(">" + sequences[sequence] + "\n" + sequence + "\n")
    trimmedcontigs.close()
    
    os.system('echo "Contigs trimming ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath )
    os.system('echo "\n************************** CONTIGS TRIMMING : END **************************\n" >> '+ logFilepath)

    #return trimmed contigs file path
    return fasta_file + "_trimmed.fasta"


def scaffolding(contigs_file, referencepath, Gid, logFilepath ,maxMemory, medusa_path, medusa_scripts_path):

	os.system('echo "\n************************** SCAFFOLDING WORKFLOW : START **************************\n" >> '+ logFilepath)

	if medusa_scripts_path[-1] != '/' :
		medusa_scripts_path = medusa_scripts_path + '/'

	Maxmemoryg=Get_maxMemoryg(maxMemory) 
	scaffold_file = Gid + "_scaffold.fasta"
	
	os.system('echo "Scaffolding started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath )

	cmd = "java " + Maxmemoryg + " -jar " + medusa_path + " -d -w2 -scriptPath " + medusa_scripts_path + \
		" -i " + contigs_file + " -o " + scaffold_file + " -f " + referencepath + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)	

	os.system('echo "Scaffolding ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath )

	os.system('echo "\n************************** SCAFFOLDING WORKFLOW : END **************************\n" >> '+ logFilepath)


	return scaffold_file	

"""
def get_scaffolding_reference(genus, MLST_id,logFilepath, report): 

	logFile = open(logFilepath, "a")
	logFile.write("Get scaffolding reference...")

	client = MongoClient('localhost', 27017)
	db = client.GAMeRdb
	reference = db.REFERENCE

	os.mkdir("reference_scaffolding")

	if(genus == "Salmonella"):
		if (reference.find({"Phylogeny.Serovar":MLST_id , "Phylogeny.Genus":genus}).count() > 0): 
			reference = reference.find({"Phylogeny.Serovar":MLST_id , "Phylogeny.Genus":genus})
			for element in reference :
				referenceZIP = element["ZIPpath"]
				break
			reference = referenceZIP	

		else :
			reference = "/mnt/NAS/NASBIO1/DATA/REFERENCES/Salmonella/Typhimurium_LT2_AE006468.zip"

	if(genus == "Listeria"):
		MLST_id = MLST_id[2:]
		if (reference.find({"Phylogeny.ClonalComplex":MLST_id , "Phylogeny.Genus":genus}).count() > 0): 

			reference = reference.find({"Phylogeny.ClonalComplex":MLST_id , "Phylogeny.Genus":genus})
			for element in reference :
				referenceZIP = element["ZIPpath"]
				break
			reference = referenceZIP	

		else :
			reference = "/mnt/NAS/NASBIO1/DATA/REFERENCES/Listeria/EGD-e.zip"


	if(genus == "Staphylococcus"):
		MLST_id = MLST_id[0]
		if (reference.find({"Phylogeny.SequenceType":MLST_id , "Phylogeny.Genus":genus}).count() > 0):		

			reference = reference.find({"Phylogeny.SequenceType":MLST_id , "Phylogeny.Genus":genus})
			for element in reference :
				referenceZIP = element["ZIPpath"]
				break
			reference = referenceZIP

		else :
			reference = "/mnt/NAS/NASBIO1/DATA/REFERENCES/Staphylococcus/Staphylococcus_aureus_NC_007795.zip"	


	os.system("unzip " + reference + ' -d reference_scaffolding/. "*.fasta"')		
	os.system("rm reference_scaffolding/*_prot.fasta")
			
	reference_fasta = "reference_scaffolding/"+str(subprocess.check_output("ls reference_scaffolding/",shell=True))
	fasta_ref_path= str(reference_fasta).rsplit('/',1)[0]+"/" # reference path for MeDuSa option -f

	logFile.write("...ok\n")
	logFile.write("...scaffolding reference : " + reference_fasta)
	logFile.close()

	os.mkdir("reference_gff")
	os.system("unzip " + reference + ' -d reference_gff/. "*.gff*"')
	reference_gff = "reference_gff/"+str(subprocess.check_output("ls reference_gff/",shell=True))


	report_file = open(report, "a")
	report_file.write("\nReference used for scaffolding : " + reference_fasta.split('/')[-1] + '\n')
	report_file.close()


	return reference_fasta,fasta_ref_path,reference_gff.rstrip()
"""

def find_scaffoldingRef_with_Mash(assembly, genus, report, logFilepath, NASBIO1_path, nbThreads):

	logFile = open(logFilepath, "a")
	logFile.write("Get scaffolding reference with Mash...")

	os.system("mkdir mash_genome_db")
	os.mkdir("reference_scaffolding")
	os.mkdir("reference_gff")
	os.system("cp " + NASBIO1_path + "/DATA/REFERENCES/" + genus + "/*zip mash_genome_db/.")
	os.system("unzip \'mash_genome_db/*.zip\' \'*.fasta\' -d mash_genome_db/")
	os.system("unzip \'mash_genome_db/*.zip\' \'*.gff3\' -d reference_gff/")
	os.system("rm mash_genome_db/*zip mash_genome_db/*_prot.fasta")

	list_FASTA = os.listdir('mash_genome_db/')
	listdir = " mash_genome_db/".join(list_FASTA)

	print("mash sketch -o mash_genome_db/references mash_genome_db/" + listdir)
	os.system("mash sketch -o mash_genome_db/references mash_genome_db/" + listdir)

	os.system("mash dist -p " + str(nbThreads) + " mash_genome_db/references.msh " + assembly + \
		" > mash_genome_db/mash_result.tsv")

	os.system("cat mash_genome_db/mash_result.tsv >> " + logFilepath)

	mash_resultFile = open("mash_genome_db/mash_result.tsv",'r')
	lines = mash_resultFile.readlines()
	mash_resultFile.close()

	genome_ref = ""
	minDist = 1.0

	for line in lines :
		
		line = line.rstrip().split('\t')
		genome = line[0]
		dist = float(line[2])

		if dist < minDist :
			minDist = dist
			genome_ref = genome.split('/')[-1]


	os.system("cp mash_genome_db/" + genome_ref + " reference_scaffolding/.")		
	reference_fasta = "reference_scaffolding/" + genome_ref
	reference_gff = "reference_gff/" + genome_ref.split('.')[0] + ".gff3"	

	report_file = open(report, "a")
	report_file.write("\nReference used for scaffolding : " + genome_ref + \
		" (mash distance : " + str(minDist) + ")" + '\n')
	report_file.close()


	return reference_fasta,"reference_scaffolding/",reference_gff


def CloseGaps(scaffold, contigs, read1, read2, nbThreads, Gid, readsLen, logFilepath, report):

	os.system('echo "\n************************** CLOSING GAPS : START **************************\n" >> '+ logFilepath)

	#os.system("gunzip " + read1)
	#os.system("gunzip " + read2)

	#read1 = '.'.join(read1.split('.')[0:-1])
	#read2 = '.'.join(read2.split('.')[0:-1])

	os.system('echo "GMcloser started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)
	
	cmd = "gmcloser -t " + scaffold + " -q " + contigs + " -l " + str(readsLen) + \
	" -p gmclos -r " + read1 + " " + read2 + " -n " + str(nbThreads) + " -bq phred33 "

	cmd = cmd + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	os.system('echo "GMcloser ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	###################

	CloseGapsFile = Gid + "_scaffold_closed.fasta"

	if os.path.isfile("gmclos.gapclosed.fa"):
		#rename gapclosed output file
		os.system("mv gmclos.gapclosed.fa " + CloseGapsFile)

	else :
		os.system('echo "ERROR GMcloser no result"  ')	
		CloseGapsFile = scaffold
		rep = open(report, 'a')
		rep.write("\nERROR GMcloser --> step skipping\n")
		rep.close()


	###################

	os.system("rm -r gmclos*")	

	os.system('echo "\n************************** CLOSING GAPS : END **************************\n" >> '+ logFilepath)
	return CloseGapsFile


def rename_contig(fasta_file, genome_id, logFilepath):

	os.system('echo "\n************************** RENAME CONTIGS : START **************************\n" >> '+ logFilepath)

	fasta = open(fasta_file, 'r')
	fasta_lines = fasta.readlines()
	fasta.close()

	rename_fastaFile_name = genome_id + "_assembly.fasta"
	rename_fastaFile = open(rename_fastaFile_name, 'w')

	i = 1
	for line in fasta_lines :
		if line[0] == '>':
			rename_fastaFile.write('>' + genome_id + '_' + str(i) + '\n')
			i+=1
		else :
			j = 1
			for n in line :

				if j%70==0 :
					rename_fastaFile.write('\n')
			
				rename_fastaFile.write(n)
			
				j+=1	

	rename_fastaFile.close()

	os.system('echo "\n************************** RENAME CONTIGS : END **************************\n" >> '+ logFilepath)
	
	return rename_fastaFile_name	


def annotation(fasta_file, genome_id, genus, nbThreads, logFilepath):

	os.system('echo "\n************************** GENOME ANNOTATION : START **************************\n" >> '+ logFilepath)

	cmd = "prokka --fast --genus " + genus + " --prefix annotation" + \
	" -cpus " + str(nbThreads) + " " + fasta_file
	cmd = cmd + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	annotation_gbk = genome_id + ".gbk"
	annotation_gff = genome_id + ".gff"

	os.system("mv annotation/annotation.gbk " + annotation_gbk)
	os.system("mv annotation/annotation.gff " + annotation_gff)

	os.system('echo "\n************************** GENOME ANNOTATION : END **************************\n" >> '+ logFilepath)

	return annotation_gbk, annotation_gff


def get_len(fastqc_txt):

    readslength = subprocess.check_output("grep 'Sequence length' " + fastqc_txt + " | cut -f2 ",shell=True).rstrip('\n')
    readslength = readslength.split('-')[-1]
    return readslength

	
def check_scaffold_coverage(scaffold, reference, min_percent, nbThreads, report, log, max_percent_diff):

	fasta_dict = SeqIO.to_dict(SeqIO.parse(scaffold, "fasta"))
	fasta_len_dict = {}
	sum_scaffold_len = 0

	for seq in fasta_dict :
		scaffold_len = len(str(fasta_dict[seq].seq))
		fasta_len_dict[seq] = scaffold_len
		sum_scaffold_len += scaffold_len

	ref_dict = SeqIO.to_dict(SeqIO.parse(reference, "fasta"))
	sum_reference_len = 0
	for seq in ref_dict :
		ref_len = len(str(ref_dict[seq].seq))
		sum_reference_len += ref_len


	diff_longer = abs(100 - (float(sum_scaffold_len)/float(sum_reference_len) * 100))

	if (diff_longer > max_percent_diff) :
		logFile = open(log, 'a')
		reportFile = open(report, 'a')
		error = "ERROR : assembly length too different from the reference length " + \
		"(ref:" + str(sum_reference_len) + "pb | assembly:" + str(sum_scaffold_len) + "pb) !!!"
		print(error)
		logFile.write(error)
		reportFile.write(error)
		logFile.close()
		reportFile.close()

		os.system("rm -r temp reference_scaffolding Assembly \
			fastqc contigs.fasta* scaffolds.fasta stats mash_genome_db annotation trashReads.fastq \
			reference_gff *_scaffold_closed.fasta* *_paired_R*.fq *_R*.fastq *_iVARCAll")

		sys.exit(1)


	tmp_sam_file = "check_scaffold_coverage.sam"	

	os.system("bwa index " + reference)
	os.system("bwa mem -t " + str(nbThreads) + " " + reference + " " + scaffold + \
		" > " + tmp_sam_file)	

	os.system("rm " + reference + ".*")

	tmp_sam_file2 = "check_scaffold_coverage_mapped.sam"
	os.system("samtools view -S -F 4 " + tmp_sam_file + " > " + tmp_sam_file2)

	sam_file = open(tmp_sam_file2, 'r')
	lines = sam_file.readlines()
	sam_file.close()

	sum_mapped_scaffold = 0
	scaffold_mapped_lit = []

	for line in lines :

		if line[0] == '@' :
			continue

		scaffoldId = line.split('\t')[0]	

		if scaffoldId not in scaffold_mapped_lit :
			scaffold_mapped_lit.append(scaffoldId)
			sum_mapped_scaffold += fasta_len_dict[scaffoldId]

	os.remove(tmp_sam_file)
	os.remove(tmp_sam_file2)
	print(sum_mapped_scaffold)
	print(sum_scaffold_len)
	mapped_percent = float(sum_mapped_scaffold) / float(sum_scaffold_len) * 100.0

	if mapped_percent >= min_percent :
		return

	else :
		logFile = open(log, 'a')
		reportFile = open(report, 'a')
		error = "ERROR : only " + str(mapped_percent) + " percent map on " + reference + " !!!"
		print(error)
		logFile.write(error)
		reportFile.write(error)
		logFile.close()
		reportFile.close()

		os.system("rm -r temp reference_scaffolding Assembly \
			fastqc contigs.fasta* scaffolds.fasta stats mash_genome_db annotation trashReads.fastq \
			reference_gff *_scaffold_closed.fasta* *_paired_R*.fq *_R*.fastq *_iVARCAll")

		sys.exit(1)


def make_mash_sketch(assembly, nbThreads, genome_id, Kmer_size, Sketch_size, logFilepath):

	cmd = "mash sketch -p " + str(nbThreads) + " -k " + Kmer_size + " -s " + Sketch_size + " -r -o " + genome_id + " " + assembly
	cmd = cmd + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	return genome_id + ".msh"


def parse_reports(SampleID, cwd):

	artwork_report =  cwd + '/' + SampleID + "_report.txt"
	quast_report = cwd + "/QUAST/report.tsv"

	Dico_infos = {}

	#Parse ARTwort report
	artwork_file = open (artwork_report, 'r')
	artwork_content = artwork_file.readlines()
	artwork_file.close()

	for line in artwork_content :
		line = line.rstrip()
		line = line.split(' : ')
		# get ref_SNP
		if line[0] == 'REFERENCE GENOME' :
			Dico_infos["RefSNP"] = line[1]

    	# get breadth_cov
		elif line[0] == 'BREADTH COVERAGE' :
			Dico_infos["BreadthCoverage"] = line[1].split('%')[0]

    	# get ref_scafold
		elif line[0] == 'Reference used for scaffolding' :
			Dico_infos["RefScafold"] = line[1].split('.')[0]

	#Parse QUAST report
	quast_file = open (quast_report, 'r')
	quast_content = quast_file.readlines()
	quast_file.close()

	for line in quast_content :
		line = line.rstrip()
		line = line.split('\t')

	    # get nb contigs
		if line[0] == '# contigs (>= 0 bp)' :
			Dico_infos["NbContigs"] = line[1]

	    # get total_length
		elif line[0] == 'Total length (>= 0 bp)' :
			Dico_infos["TotalLength"] = line[1]

		# get largest_contig
		elif line[0] == 'Largest contig' :
			Dico_infos["LargestContig"] = line[1]

	    # get N50
		elif line[0] == 'N50' :
			Dico_infos["N50"] = line[1]

    	# genome_fraction
		elif line[0] == 'Genome fraction (%)' :
			Dico_infos["GenomeFraction"] = line[1]

	return Dico_infos


#main function	
def main():	


	##########################################
	#			Initialisation				 #
	##########################################

	# Processing time counter
	t0 = time.time()
	
	# Get arguments 
	parser=get_parser()
	
	# Print parser.help if no arguments
	if len(sys.argv)==1:
		parser.print_help()
		sys.exit(1)
	
	Arguments=parser.parse_args()	
	
	#itsmeludo 23/12/2018
        
        if not Arguments.medusa_path:
                medusa_path=os.path.join(os.environ['CONDA_PREFIX'],"bin","medusa.jar")
        else:
                medusa_path=Arguments.medusa_path
        if not Arguments.medusa_scripts_path:
                medusa_scripts_path=os.path.join(os.environ['CONDA_PREFIX'],"bin","medusa_scripts")
        else:
                medusa_scripts_path=Arguments.medusa_scripts_path
        ###fin modif
	#itsmeludo 27/12/2018
        if not Arguments.GATK_path:
                gatk_path=os.path.join(os.environ['CONDA_PREFIX'],"bin","GenomeAnalysisTK.jar")
        else:
                gatk_path=Arguments.GATK_path
	###fin modif	
    
	print("Step 1/16 ---> Initialisation")

	# Go to working directory gived by Arguments.workdir
	if(Arguments.workdir != '.'):
		os.chdir(Arguments.workdir)	

	cwd = os.getcwd() #Current working directory FULL path	

	# GLOBAL VARIABLES
	GENOME_ID = '_'.join(Arguments.reads1.split('/')[-1].split('.')[0].split('_')[0:-1])
	LOG_FILE = cwd + "/" + GENOME_ID + ".log"
	REPORT_FILE = GENOME_ID + "_report.txt"
	if Arguments.NASBIO1_PATH[-1] == '/' :
		Arguments.NASBIO1_PATH = Arguments.NASBIO1_PATH[0:-1]
	FINAL_OUTPUT_DIRECTORY = Arguments.NASBIO1_PATH + "/DATA/GAMeR_DB/" + Arguments.genus.upper() + '/' + GENOME_ID
	ADAPTATERS_FILE = Arguments.NASBIO1_PATH + "/DATA/all_known_iontorrent-Illumina.fa"

	#Retrieve Python command
	command=""
	for arg in sys.argv:
		command = command + " " + arg	

	# First write in Log File : date and command
	LOG = open(LOG_FILE, 'w')
	LOG.write("READSPROCESS STARTED :" + time.strftime("%d/%m/%Y") + ' ' + time.strftime("%H:%M:%S") + '\n\n')
	LOG.write("COMMAND USED :" + command + '\n\n\n')
	LOG.close()

	# Check GENOME_ID
	check_finalDirectory_exist(FINAL_OUTPUT_DIRECTORY, LOG_FILE)
	check_MongoDB_exist(GENOME_ID, Arguments.genus, LOG_FILE, Arguments.MongoUser, Arguments.MongoPassword, Arguments.MongoServer)

	# Make final directory 
	os.makedirs(FINAL_OUTPUT_DIRECTORY)

	# First write in Report file : Metadata and key processing parameters
	REPORT = open(REPORT_FILE, 'w')
	REPORT.write("++++++++++++++++++++++++++++++++++++++++ DATASET REPORT  ++++++++++++++++++++++++++++++++++++++++\n" + \
	"GENERATED " + time.strftime("%d/%m/%Y") + ' AT ' + time.strftime("%H:%M:%S") + '\n' + \
	"READS : \n" + Arguments.reads1.split('/')[-1] + "\n" + Arguments.reads2.split('/')[-1] + '\n' + \
	"GENUS : " + Arguments.genus + '\n' + \
	"SP : " + Arguments.species + '\n' + \
	"\n"+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+                 METADATA             +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n\n" + \
	"Supplied by : " + Arguments.supplier + '\n' +\
	"Sequencing center : " + Arguments.center + '\n' + \
	"Sequencing technology : " + Arguments.technology + '\n' + \
	"Projet id : " + Arguments.projetID + '\n' + \
	"\n"+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+             KEY PARAMETERS           +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n"+ \
	"Coverage threshold : " + str(Arguments.minCov) + '\n' + \
	"Contig length threshold : " + str(Arguments.minContigLen) + '\n' + \
	"Phred score threshold : " + str(Arguments.minPhredScore) + '\n' + \
	"\n"+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+              MAIN METRICS            +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n")
	REPORT.close()


	##########################################
	#				Check reads				 #
	##########################################
	
	print("Step 2/16 ---> Check reads format")

	### READ 1 ###

	# If file isn't in the right format, analyse it and uncompress it if possible. 
	if(Arguments.reads1.split('.')[-1] not in ["fastq", "fq", "FASTQ", "FQ"]):
		Arguments.reads1,IS_corrupted = uncompress_in_workdir(Arguments.reads1, LOG_FILE,cwd)

		if (IS_corrupted==True):
			# ASTUCE : utiliser booleen IS_corrupted: En considerant que le script sera appelé pour chaque paire de reads, sauter le traitement du reads pair si le premier est déjà corrompu
			sys.exit(1) 

	# File is already in the right format : copy it in the current work directory		
	else:
		copyfile(Arguments.reads1, cwd + "/" + list(Arguments.reads1.rsplit('/',1))[1])
		Arguments.reads1= cwd + '/' + Arguments.reads1.split('/')[-1]
		LOG=open(LOG_FILE,"a")
		LOG.write("Reads already uncompressed in:" + Arguments.reads1 + "\n")
		LOG.close()


	### READ 2 : Same procedure (if read1 processed successfully, according to IS_corrupted ) ###


	if(Arguments.reads2.split('.')[-1] not in ["fastq", "fq", "FASTQ", "FQ"]):
		Arguments.reads2,IS_corrupted = uncompress_in_workdir(Arguments.reads2, LOG_FILE,cwd)
		
		if (IS_corrupted==True):
			sys.exit(1)

	else:
		copyfile(Arguments.reads2, cwd + "/" + list(Arguments.reads2.rsplit('/',1))[1])
		Arguments.reads2 = cwd + '/' + Arguments.reads2.split('/')[-1]	
		LOG=open(LOG_FILE,"a")
		LOG.write("reads already uncompressed in:" + Arguments.reads2)
		LOG.close()	


	##########################################
	#		    Coverage checking			 #
	##########################################
	
	print("Step 3/16 ---> quality control")
	coverage = Check_coverage(Arguments.reads1,Arguments.reads2,Arguments.genus,Arguments.minCov,Arguments.nbThreads,Arguments.maxMemory,\
	cwd,LOG_FILE,REPORT_FILE, Arguments.NASBIO1_PATH)


	##########################################
	#		      Normalization 			 #
	##########################################
	
	print("Step 4/17 ---> normalization")
	if(coverage > Arguments.targetCov):
		reads1, reads2 = normalization(Arguments.reads1, Arguments.reads2, Arguments.targetCov, Arguments.nbThreads, GENOME_ID, LOG_FILE, REPORT_FILE)
		os.system("mv " + reads1 + " " + Arguments.reads1)
		os.system("mv " + reads2 + " " + Arguments.reads2)

	##########################################
	#		    	  FASTQC		    	 #
	##########################################

	print("Step 5/17 ---> FASTQC")

	fastqcResult = Fastqc(Arguments.reads1, Arguments.reads2, ADAPTATERS_FILE, "fastqc", Arguments.nbThreads, cwd, LOG_FILE, REPORT_FILE)

	FASTQC_R1 = fastqcResult[0]
	FASTQC_R2 = fastqcResult[1]
	fastqc_txtreport = fastqcResult[2]


	##########################################
	#		    	iVARCall		    	 #
	##########################################

	print("Step 6/17 ---> iVARCall2")

	VCF_FILE = commande_iVARCAll(Arguments.genus, Arguments.reads1,Arguments.reads2, ADAPTATERS_FILE,\
		Arguments.nbThreads, Arguments.maxMemory, GENOME_ID,cwd, LOG_FILE, REPORT_FILE, Arguments.NASBIO1_PATH, \
		GATK_path=gatk_path)
#	VCF_FILE = commande_iVARCAll(Arguments.genus, Arguments.reads1,Arguments.reads2, ADAPTATERS_FILE,\
#                Arguments.nbThreads, Arguments.maxMemory, GENOME_ID,cwd, LOG_FILE, REPORT_FILE, Arguments.NASBIO1_PATH, \
#                Arguments.trimmomatic_path, Arguments.GATK_path, Arguments.picard_path)


	##########################################
	#		    Trimming reads		    	 #
	##########################################

	print("Step 7/17 ---> Trimming")
	reads_trim =  soft_trimming(Arguments.reads1, Arguments.reads2, ADAPTATERS_FILE, Arguments.nbThreads, Arguments.maxMemory, \
	Arguments.minPhredScore, GENOME_ID, cwd, REPORT_FILE, LOG_FILE)
	
	reads1_trim = reads_trim[0]
	reads2_trim = reads_trim[1]


	##########################################
	#	        De novo assembly        	 #
	##########################################	

	print("Step 8/17 ---> De novo assembly")

	CONTIGS = Assembly(reads1_trim,reads2_trim,cwd,LOG_FILE,Arguments.nbThreads)


	##########################################
	#	              MLST	    	    	 #
	##########################################

	print("Step 9/17 ---> MLST")

	if(Arguments.genus != "Clostridium" and Arguments.genus != "Bacillus"):
		MLST_result = MSLT_analysis(CONTIGS, Arguments.nbThreads, LOG_FILE, REPORT_FILE, Arguments.genus, Arguments.NASBIO1_PATH, cwd)
		RESULT_ST = MLST_result[0]

		if(Arguments.genus == "Salmonella"):
			#RESULT_SEROVAR = MLST_result[1]
			#RESULT_ANTIGENIC_PROFILE = MLST_result[2]
			#MLST_id = [RESULT_SEROVAR, RESULT_ANTIGENIC_PROFILE]
			pass

		elif(Arguments.genus == "Listeria"):
			RESULT_CC = MLST_result[1]	
			MLST_id = RESULT_CC

		elif(Arguments.genus == "Staphylococcus"):
			MLST_id = RESULT_ST

	else:
		MLST_id = "0"	
		RESULT_ST = "0"


	##########################################
	#	           Scaffolding   	         #
	##########################################

	print("Step 10/17 ---> Scaffolding")

	#new_reference,new_reference_directory, GFFref = get_scaffolding_reference(Arguments.genus, MLST_id,LOG_FILE, REPORT_FILE)
	new_reference,new_reference_directory, GFFref = find_scaffoldingRef_with_Mash(CONTIGS, Arguments.genus, \
	REPORT_FILE, LOG_FILE, Arguments.NASBIO1_PATH, Arguments.nbThreads)
	scaffold = scaffolding(CONTIGS, new_reference_directory, GENOME_ID, LOG_FILE, Arguments.maxMemory, \
	medusa_path , medusa_scripts_path)


	##########################################
	#	          Gap Closing	             #
	##########################################	

	print("Step 11/17 ---> Gap closing")

	readsLen = get_len(str(fastqc_txtreport))
	scaffold = CloseGaps(scaffold, CONTIGS, reads1_trim, reads2_trim, Arguments.nbThreads, GENOME_ID, readsLen, LOG_FILE, REPORT_FILE)

	##########################################
	#	         Contig filtration     	 	 #
	##########################################

	print("Step 12/17 ---> Contig filtration")

	scaffold = TrimContigsByLen(scaffold, Arguments.minContigLen, LOG_FILE)


	##########################################
	#	      QUAST quality control     	 #
	##########################################

	print("Step 13/17 ---> Assembly control")

	check_scaffold_coverage(scaffold, new_reference, Arguments.min_mapped_scaffold_percent, Arguments.nbThreads, REPORT_FILE, LOG_FILE, \
	Arguments.max_len_diff)
	QUAST = Assembly_control(scaffold, new_reference, Arguments.minContigLen, GFFref, cwd, LOG_FILE, Arguments.nbThreads)
	

	##########################################
	#	        Rename scaffolds           	 #
	##########################################

	print("Step 14/17 ---> Rename scaffold and make mash sketch")

	ASSEMBLY = rename_contig(scaffold, GENOME_ID, LOG_FILE)

	SKETCH = make_mash_sketch(ASSEMBLY, Arguments.nbThreads, GENOME_ID, Arguments.Ksize, Arguments.Ssize, LOG_FILE)


	##########################################
	#	        	Annotation            	 #
	##########################################

	print("Step 15/17 ---> Genome annotation")

	GBK, GFF = annotation(ASSEMBLY, GENOME_ID, Arguments.genus, Arguments.nbThreads, LOG_FILE)


	##########################################
	#	          Save results          	 #
	##########################################

	print("Step 16/17 ---> Moving files and add to mongoDB")

	dico_Suplinfos = parse_reports(GENOME_ID, cwd)
	
	if Arguments.genus == "Salmonella" :
		RESULT_SEROVAR,RESULT_ANTIGENIC_PROFILE = get_serovar_from_ST_Salmonella(ASSEMBLY,LOG_FILE, REPORT_FILE, cwd)
		MLST_id = [RESULT_SEROVAR, RESULT_ANTIGENIC_PROFILE]

	sampleObjet = sample(GENOME_ID, Arguments.supplier, Arguments.projetID, REPORT_FILE, \
		Arguments.reads1, Arguments.reads2, readsLen, FASTQC_R1, FASTQC_R2, Arguments.center, \
		Arguments.technology, ASSEMBLY, CONTIGS, QUAST, GBK, GFF, Arguments.genus, \
		Arguments.species, RESULT_ST, MLST_id, VCF_FILE, SKETCH, dico_Suplinfos['NbContigs'], \
		dico_Suplinfos['TotalLength'], dico_Suplinfos['LargestContig'], \
		dico_Suplinfos['N50'], dico_Suplinfos['GenomeFraction'], dico_Suplinfos['RefScafold'], dico_Suplinfos['RefSNP'], \
		dico_Suplinfos['BreadthCoverage'])

	sampleObjet.moveFiles(FINAL_OUTPUT_DIRECTORY)
	sampleObjet.insertMongo(Arguments.MongoUser, Arguments.MongoPassword, Arguments.MongoServer)


	##########################################
	#	        Remove tmp files	       	 #
	##########################################

	print("Step 17/17 ---> Cleaning temporary files")

	# Last write in log file : date and time elapsed
	t1 = time.time() - t0
	LOG = open(LOG_FILE, 'a')
	LOG.write(" \n\n\n READSPROCESS ENDED :" + time.strftime("%d/%m/%Y") + ' ' + time.strftime("%H:%M:%S") + "\n\n")
	LOG.write(" TIME ELAPSED (hour:min:sec) :" + str(datetime.timedelta(seconds=int(t1))))
	LOG.close()

	os.system("mv " + GENOME_ID + ".log " + FINAL_OUTPUT_DIRECTORY + "/.")

	# Remove tmp files and directories
	os.system("rm -r  " + GENOME_ID + "* temp reference_scaffolding Assembly \
		fastqc contigs.fasta* scaffolds.fasta stats mash_genome_db annotation trashReads.fastq \
		reference_gff")


if __name__ == "__main__":
	main()	        		
